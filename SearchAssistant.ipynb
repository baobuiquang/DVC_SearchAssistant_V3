{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39065967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"static/DATA.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#     DATA = json.load(f)\n",
    "# ldg_names = [e[\"name\"] for e in DATA[\"DVC_TTHC_LamDong\"][\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff17d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_PASSAGES = [\"Thủ tục thành lập công ty tư nhân\", \"Thủ tục đăng ký kết hôn\", \"Thủ tục chuyển nhượng quyền sử dụng đất\", \"Thủ tục đấu thầu đất xây dựng\", \"Thủ tục cấp lại lý lịch tư pháp\", \"Thủ tục chuyển trường cho học sinh trung học phổ thông\", \"Thủ tục chuyển trường cho học sinh trung học cơ sở\", \"Thủ tục chuyển trường cho học sinh tiểu học\", \"Thủ tục đăng ký lại kết hôn\", \"Thủ tục đăng ký kết hôn có yếu tố nước ngoài\", \"Thủ tục làm giấy khai sinh\", \"Thủ tục thành lập công ty trách nhiệm hữu hạn 1 thành viên\", \"Thủ tục thành lập công ty trách nhiệm hữu hạn 2 thành viên trở lên\", \"Thủ tục tố cáo tại cấp xã\", \"Thủ tục tố cáo tại cấp tỉnh\"]\n",
    "_TEST_QUERIES = [\"Cháu muốn chuyển trường cấp 3 thì cần phải làm gì?\", \"Tôi muốn mở công ty thì thủ tục gì?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7dd27",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edb4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download as HF_Download\n",
    "from tokenizers import Tokenizer as STL_Tokenizer\n",
    "from rank_bm25 import BM25Okapi as BM25_Retriever\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pkg.NLPT.NLPT import Process_NLPT_Tokenize, Process_NLPT_Normalize\n",
    "os.makedirs(\"_hyse\", exist_ok=True)\n",
    "\n",
    "def dict2json(dict, jsonpath):\n",
    "    try:\n",
    "        with open(jsonpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict, f, ensure_ascii=False, indent=4)\n",
    "    except Exception as er:\n",
    "        print(f\"⚠️ dict2json > Error: {er}\")\n",
    "\n",
    "def json2dict(jsonpath):\n",
    "    dict = {}\n",
    "    try:\n",
    "        with open(jsonpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            dict = json.load(f)\n",
    "    except Exception as er:\n",
    "        print(f\"⚠️ json2dict > Error: {er}\")\n",
    "    return dict\n",
    "\n",
    "class SentenceTransformerLite:\n",
    "    # Init: model_path -> model + tokenizer\n",
    "    def __init__(self, model_path=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        try:\n",
    "            # Model (ONNX)\n",
    "            try: HF_Download(repo_id=model_path, filename=\"onnx/model.onnx_data\")\n",
    "            except: pass\n",
    "            STL_model = ort.InferenceSession(HF_Download(repo_id=model_path, filename=\"onnx/model.onnx\"))\n",
    "            # Tokenizer\n",
    "            STL_tokenizer = STL_Tokenizer.from_pretrained(model_path)\n",
    "            STL_tokenizer.enable_padding(pad_id=1, pad_token=\"<pad>\")\n",
    "            STL_tokenizer.enable_truncation(max_length=512)\n",
    "        except Exception as er:\n",
    "            raise ValueError(f\"⚠️ > SentenceTransformerLite > init > Error: {er}\")\n",
    "        # Return\n",
    "        self.STL_model = STL_model\n",
    "        self.STL_tokenizer = STL_tokenizer\n",
    "    # Encode: Text(s) -> Embedding(s)\n",
    "    def encode(self, inputtexts):\n",
    "        # Ensure inputtexts is a list of strings\n",
    "        if isinstance(inputtexts, list) and all(isinstance(e, str) for e in inputtexts):\n",
    "            if len(inputtexts) == 0:\n",
    "                raise ValueError(f\"⚠️ > SentenceTransformerLite > encode > inputtexts = empty list []\")\n",
    "        elif isinstance(inputtexts, str):\n",
    "            inputtexts = [inputtexts]\n",
    "        else:\n",
    "            raise ValueError(f\"⚠️ > SentenceTransformerLite > encode > inputtexts != string or list of strings\")\n",
    "        # Tokenize\n",
    "        inputs = self.STL_tokenizer.encode_batch(inputtexts, is_pretokenized=False)\n",
    "        inputs_ids = np.array([e.ids for e in inputs], dtype=np.int64)\n",
    "        inputs_msk = np.array([e.attention_mask for e in inputs], dtype=np.int64)\n",
    "        # Encoding\n",
    "        embeddings = self.STL_model.run(None, {\"input_ids\": inputs_ids, \"attention_mask\": inputs_msk})[0]                                             # Encode\n",
    "        embeddings = np.sum(embeddings * np.expand_dims(inputs_msk, axis=-1), axis=1) / np.maximum(np.sum(inputs_msk, axis=1, keepdims=True), 1e-9)   # Pooling\n",
    "        embeddings = embeddings / np.maximum(np.linalg.norm(embeddings, axis=1, keepdims=True), 1e-9)                                                 # Normalize\n",
    "        # Return\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcc1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineSemantic:\n",
    "    # ----- Example -----\n",
    "    # engine_semantic = EngineSemantic()\n",
    "    # engine_semantic.update(_TEST_PASSAGES)\n",
    "    # engine_semantic.search(_TEST_QUERIES)\n",
    "    # -------------------\n",
    "    def __init__(self, name=\"hyse1_sem1\", modelpath=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        self.name = name\n",
    "        self.modelpath = modelpath\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.npy\"\n",
    "        self.model = SentenceTransformerLite(modelpath)\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # 📤 Read file as docs\n",
    "            self.embs = np.load(self.savepath_embs)            # 📤 Read file as embs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            pass\n",
    "        else:\n",
    "            self.docs = new_docs\n",
    "            self.embs = self.model.encode(self.docs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # 📥 Save docs as file\n",
    "            np.save(self.savepath_embs, self.embs)             # 📥 Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        embs_queries = self.model.encode(new_queries)\n",
    "        # -----\n",
    "        similarities = embs_queries @ self.embs.T\n",
    "        best_matching_idxs = [[idx for idx, _ in sorted(enumerate(sim), key=lambda x: x[1], reverse=True)][:min(top, len(self.docs))] for sim in similarities]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        return [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineLexical:\n",
    "    def __init__(self, name=\"hyse1_lex1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        self.model = None\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # 📤 Read file as docs\n",
    "            self.embs = json2dict(self.savepath_embs)[\"embs\"]  # 📤 Read file as embs\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            pass\n",
    "        else:\n",
    "            self.docs = new_docs\n",
    "            self.embs = [Process_NLPT_Tokenize(e) for e in self.docs]\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # 📥 Save docs as file\n",
    "            dict2json({\"embs\": self.embs}, self.savepath_embs) # 📥 Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        queries_embs = [Process_NLPT_Tokenize(e) for e in new_queries]\n",
    "        similarities = [self.model.get_scores(query_emb) for query_emb in queries_embs]\n",
    "        best_matching_idxs = [self.model.get_top_n(query_emb, range(len(self.docs)), n=top) for query_emb in queries_embs]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        return [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8759bc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m engine_semantic = EngineLexical()\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# engine_semantic.update(_TEST_PASSAGES)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mengine_semantic\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_TEST_QUERIES\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mEngineLexical.search\u001b[39m\u001b[34m(self, new_queries, top)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_queries, top=\u001b[32m5\u001b[39m):\n\u001b[32m     23\u001b[39m     queries_embs = [Process_NLPT_Tokenize(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m new_queries]\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     similarities = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_emb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquery_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mqueries_embs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     25\u001b[39m     best_matching_idxs = [\u001b[38;5;28mself\u001b[39m.model.get_top_n(query_emb, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.docs)), n=top) \u001b[38;5;28;01mfor\u001b[39;00m query_emb \u001b[38;5;129;01min\u001b[39;00m queries_embs]\n\u001b[32m     26\u001b[39m     best_matching_docs = [[\u001b[38;5;28mself\u001b[39m.docs[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m e] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m best_matching_idxs]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_queries, top=\u001b[32m5\u001b[39m):\n\u001b[32m     23\u001b[39m     queries_embs = [Process_NLPT_Tokenize(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m new_queries]\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     similarities = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_scores\u001b[49m(query_emb) \u001b[38;5;28;01mfor\u001b[39;00m query_emb \u001b[38;5;129;01min\u001b[39;00m queries_embs]\n\u001b[32m     25\u001b[39m     best_matching_idxs = [\u001b[38;5;28mself\u001b[39m.model.get_top_n(query_emb, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.docs)), n=top) \u001b[38;5;28;01mfor\u001b[39;00m query_emb \u001b[38;5;129;01min\u001b[39;00m queries_embs]\n\u001b[32m     26\u001b[39m     best_matching_docs = [[\u001b[38;5;28mself\u001b[39m.docs[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m e] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m best_matching_idxs]\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'get_scores'"
     ]
    }
   ],
   "source": [
    "engine_semantic = EngineLexical()\n",
    "# engine_semantic.update(_TEST_PASSAGES)\n",
    "engine_semantic.search(_TEST_QUERIES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
