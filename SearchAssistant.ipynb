{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39065967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"static/DATA.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#     DATA = json.load(f)\n",
    "# ldg_names = [e[\"name\"] for e in DATA[\"DVC_TTHC_LamDong\"][\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_PASSAGES = [\"Thủ tục thành lập công ty tư nhân\", \"Thủ tục đăng ký kết hôn\", \"Thủ tục chuyển nhượng quyền sử dụng đất\", \"Thủ tục đấu thầu đất xây dựng\", \"Thủ tục cấp lại lý lịch tư pháp\", \"Thủ tục chuyển trường cho học sinh trung học phổ thông\", \"Thủ tục chuyển trường cho học sinh trung học cơ sở\", \"Thủ tục chuyển trường cho học sinh tiểu học\", \"Thủ tục đăng ký lại kết hôn\", \"Thủ tục đăng ký kết hôn có yếu tố nước ngoài\", \"Thủ tục làm giấy khai sinh\", \"Thủ tục thành lập công ty trách nhiệm hữu hạn 1 thành viên\", \"Thủ tục thành lập công ty trách nhiệm hữu hạn 2 thành viên trở lên\", \"Thủ tục tố cáo tại cấp xã\", \"Thủ tục tố cáo tại cấp tỉnh\"]\n",
    "_TEST_QUERIES = [\n",
    "    \"Chuyển Trường\", \n",
    "    \"Chuyen Truong\", \n",
    "    \"Khai Sinh\", \n",
    "    \"Cháu muốn chuyển trường cấp 3 thì cần phải làm gì?\", \n",
    "    \"Tôi muốn mở công ty thì thủ tục gì?\", \n",
    "    \"khởi nghiệp\", \n",
    "    \"Sắp cưới vợ cần làm gì?\", \n",
    "    \"đăng ký kết hôn\", \n",
    "    \"dĂng  kÝ  kÊt   hoN\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7dd27",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download as HF_Download\n",
    "from tokenizers import Tokenizer as STL_Tokenizer\n",
    "from rank_bm25 import BM25Okapi as BM25_Retriever\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pkg.NLPT.NLPT import Process_NLPT_Tokenize, Process_NLPT_Normalize\n",
    "os.makedirs(\"_hyse\", exist_ok=True)\n",
    "\n",
    "def dict2json(dict, jsonpath):\n",
    "    try:\n",
    "        with open(jsonpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict, f, ensure_ascii=False, indent=4)\n",
    "    except Exception as er:\n",
    "        print(f\"⚠️ dict2json > Error: {er}\")\n",
    "\n",
    "def json2dict(jsonpath):\n",
    "    dict = {}\n",
    "    try:\n",
    "        with open(jsonpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            dict = json.load(f)\n",
    "    except Exception as er:\n",
    "        print(f\"⚠️ json2dict > Error: {er}\")\n",
    "    return dict\n",
    "\n",
    "class SentenceTransformerLite:\n",
    "    # Init: model_path -> model + tokenizer\n",
    "    def __init__(self, model_path=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        try:\n",
    "            # Model (ONNX)\n",
    "            try: HF_Download(repo_id=model_path, filename=\"onnx/model.onnx_data\")\n",
    "            except: pass\n",
    "            STL_model = ort.InferenceSession(HF_Download(repo_id=model_path, filename=\"onnx/model.onnx\"))\n",
    "            # Tokenizer\n",
    "            STL_tokenizer = STL_Tokenizer.from_pretrained(model_path)\n",
    "            STL_tokenizer.enable_padding(pad_id=1, pad_token=\"<pad>\")\n",
    "            STL_tokenizer.enable_truncation(max_length=512)\n",
    "        except Exception as er:\n",
    "            raise ValueError(f\"⚠️ > SentenceTransformerLite > init > Error: {er}\")\n",
    "        # Return\n",
    "        self.STL_model = STL_model\n",
    "        self.STL_tokenizer = STL_tokenizer\n",
    "    # Encode: Text(s) -> Embedding(s)\n",
    "    def encode(self, inputtexts):\n",
    "        # Ensure inputtexts is a list of strings\n",
    "        if isinstance(inputtexts, list) and all(isinstance(e, str) for e in inputtexts):\n",
    "            if len(inputtexts) == 0:\n",
    "                raise ValueError(f\"⚠️ > SentenceTransformerLite > encode > inputtexts = empty list []\")\n",
    "        elif isinstance(inputtexts, str):\n",
    "            inputtexts = [inputtexts]\n",
    "        else:\n",
    "            raise ValueError(f\"⚠️ > SentenceTransformerLite > encode > inputtexts != string or list of strings\")\n",
    "        # Tokenize\n",
    "        inputs = self.STL_tokenizer.encode_batch(inputtexts, is_pretokenized=False)\n",
    "        inputs_ids = np.array([e.ids for e in inputs], dtype=np.int64)\n",
    "        inputs_msk = np.array([e.attention_mask for e in inputs], dtype=np.int64)\n",
    "        # Encoding\n",
    "        embeddings = self.STL_model.run(None, {\"input_ids\": inputs_ids, \"attention_mask\": inputs_msk})[0]                                             # Encode\n",
    "        embeddings = np.sum(embeddings * np.expand_dims(inputs_msk, axis=-1), axis=1) / np.maximum(np.sum(inputs_msk, axis=1, keepdims=True), 1e-9)   # Pooling\n",
    "        embeddings = embeddings / np.maximum(np.linalg.norm(embeddings, axis=1, keepdims=True), 1e-9)                                                 # Normalize\n",
    "        # Return\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HYSE_EngineSemantic:\n",
    "    # # ----- Example -----\n",
    "    # engine_semantic = HYSE_EngineSemantic()\n",
    "    # engine_semantic.update(_TEST_PASSAGES)\n",
    "    # engine_semantic.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_sem1\", modelpath=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        self.name = name\n",
    "        self.modelpath = modelpath\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.npy\"\n",
    "        self.model = SentenceTransformerLite(modelpath)\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # 📤 Read file as docs\n",
    "            self.embs = np.load(self.savepath_embs)            # 📤 Read file as embs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            print(f\"> HYSE_EngineSemantic '{self.name}' > No new docs\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"> HYSE_EngineSemantic '{self.name}' > Updating new docs...\")\n",
    "            self.docs = new_docs\n",
    "            self.embs = self.model.encode(self.docs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # 📥 Save docs as file\n",
    "            np.save(self.savepath_embs, self.embs)             # 📥 Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        embs_queries = self.model.encode(new_queries)\n",
    "        # -----\n",
    "        similarities = embs_queries @ self.embs.T\n",
    "        best_matching_idxs = [[idx for idx, _ in sorted(enumerate(sim), key=lambda x: x[1], reverse=True)][:min(top, len(self.docs))] for sim in similarities]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        res = [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "        return [[e for e in q if e[\"score\"] > 0] for q in res]\n",
    "\n",
    "class HYSE_EngineLexical:\n",
    "    # # ----- Example -----\n",
    "    # engine_lexical = HYSE_EngineLexical()\n",
    "    # engine_lexical.update(_TEST_PASSAGES)\n",
    "    # engine_lexical.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_lex1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        self.model = None\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # 📤 Read file as docs\n",
    "            self.embs = json2dict(self.savepath_embs)[\"embs\"]  # 📤 Read file as embs\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            print(f\"> HYSE_EngineLexical '{self.name}' > No new docs\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"> HYSE_EngineLexical '{self.name}' > Updating new docs...\")\n",
    "            self.docs = new_docs\n",
    "            self.embs = [Process_NLPT_Tokenize(e) for e in self.docs]\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # 📥 Save docs as file\n",
    "            dict2json({\"embs\": self.embs}, self.savepath_embs) # 📥 Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        queries_embs = [Process_NLPT_Tokenize(e) for e in new_queries]\n",
    "        # -----\n",
    "        similarities = [self.model.get_scores(query_emb) for query_emb in queries_embs]\n",
    "        best_matching_idxs = [self.model.get_top_n(query_emb, range(len(self.docs)), n=top) for query_emb in queries_embs]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        res = [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "        return [[e for e in q if e[\"score\"] > 0] for q in res]\n",
    "\n",
    "class HYSE_EngineExactMatch:\n",
    "    # # ----- Example -----\n",
    "    # engine_exactmatch = HYSE_EngineExactMatch()\n",
    "    # engine_exactmatch.update(_TEST_PASSAGES)\n",
    "    # engine_exactmatch.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_exa1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        if os.path.exists(self.savepath_docs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # 📤 Read file as docs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            print(f\"> HYSE_EngineExactMatch '{self.name}' > No new docs\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"> HYSE_EngineExactMatch '{self.name}' > Updating new docs...\")\n",
    "            self.docs = new_docs\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # 📥 Save docs as file\n",
    "    def search(self, new_queries):\n",
    "        best_matching_idxs = []\n",
    "        for q in new_queries:\n",
    "            # Exact match with diacritics\n",
    "            tmp_idxs = [i for i, d in enumerate(self.docs) if q.lower().strip() in d.lower().strip()]\n",
    "            if len(tmp_idxs) == 0:\n",
    "                # Exact match without diacritics\n",
    "                tmp_idxs = [i for i, d in enumerate(self.docs) if Process_NLPT_Normalize(q) in Process_NLPT_Normalize(d)]\n",
    "            best_matching_idxs.append(tmp_idxs)\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[round(len(new_queries[qidx])/len(doc), 3) for doc in e] for qidx, e in enumerate(best_matching_docs)]\n",
    "        # -----\n",
    "        res = [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "        return [[e for e in q if e[\"score\"] > 0] for q in res]\n",
    "\n",
    "class HYSE_EngineHybrid:\n",
    "    # # ----- Example -----\n",
    "    # hyse_engine = HYSE_EngineHybrid()\n",
    "    # hyse_engine.update(_TEST_PASSAGES)\n",
    "    # hyse_engine.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"HYSE1\"):\n",
    "        self.search_engine_1 = HYSE_EngineExactMatch(name=f\"{name}_EXA1\")\n",
    "        self.search_engine_2 = HYSE_EngineLexical(name=f\"{name}_LEX1\")\n",
    "        self.search_engine_3 = HYSE_EngineSemantic(name=f\"{name}_SEM1\", modelpath=\"onelevelstudio/ML-E5-0.3B\")\n",
    "        self.search_engine_4 = HYSE_EngineSemantic(name=f\"{name}_SEM2\", modelpath=\"onelevelstudio/MPNET-0.3B\")\n",
    "    def update(self, new_docs):\n",
    "        self.search_engine_1.update(new_docs)\n",
    "        self.search_engine_2.update(new_docs)\n",
    "        self.search_engine_3.update(new_docs)\n",
    "        self.search_engine_4.update(new_docs)\n",
    "    def search(self, new_queries):\n",
    "        res_1 = self.search_engine_1.search(new_queries)\n",
    "        res_2 = self.search_engine_2.search(new_queries)\n",
    "        res_3 = self.search_engine_3.search(new_queries)\n",
    "        res_4 = self.search_engine_4.search(new_queries)\n",
    "        res_search = []\n",
    "        for i in range(len(res_1)):\n",
    "            # ---------- Case 1: Exact match\n",
    "            if len(res_1[i]) > 0:\n",
    "                tmp_res = res_1[i]\n",
    "            # ---------- Case 2: Not exact match\n",
    "            else:\n",
    "                res_234 = res_2[i]+res_3[i]+res_4[i]\n",
    "                res_234 = [{\"index\": e[\"index\"], \"doc\": e[\"doc\"]} for e in res_234]\n",
    "                doc_counts = {}\n",
    "                for item in res_234: doc_counts[item['doc']] = doc_counts.get(item['doc'], 0) + 1\n",
    "                tmp_res = [{'index': item['index'], 'doc': doc, 'score': doc_counts[doc]} for doc, item in {d['doc']: d for d in res_234}.items()]\n",
    "                tmp_res = sorted(tmp_res, key=lambda x: (-x['score'], len(x['doc'])))\n",
    "            # ----------\n",
    "            res_search.append(tmp_res)\n",
    "        return res_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605fed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_semantic = HYSE_EngineSemantic(name=\"a\", modelpath=\"onelevelstudio/ML-E5-0.3B\")\n",
    "engine_semantic.update(_TEST_PASSAGES)\n",
    "engine_semantic.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_semantic = HYSE_EngineSemantic(name=\"b\", modelpath=\"onelevelstudio/MPNET-0.3B\")\n",
    "engine_semantic.update(_TEST_PASSAGES)\n",
    "engine_semantic.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_lexical = HYSE_EngineLexical(name=\"c\")\n",
    "engine_lexical.update(_TEST_PASSAGES)\n",
    "engine_lexical.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_exactmatch = HYSE_EngineExactMatch(name=\"d\")\n",
    "engine_exactmatch.update(_TEST_PASSAGES)\n",
    "engine_exactmatch.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyse_engine = HYSE_EngineHybrid(name=\"e\")\n",
    "hyse_engine.update(_TEST_PASSAGES)\n",
    "hyse_engine.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3651012",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_queries = [\"dĂng  kÝ  kÊt   hoN  \\n   hello   \\t\\n\\n\\t    yeah!!!\\t...   '     \\n\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
