{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39065967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"static/DATA.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#     DATA = json.load(f)\n",
    "# ldg_names = [e[\"name\"] for e in DATA[\"DVC_TTHC_LamDong\"][\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff17d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_PASSAGES = [\"Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty t∆∞ nh√¢n\", \"Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n\", \"Th·ªß t·ª•c chuy·ªÉn nh∆∞·ª£ng quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t\", \"Th·ªß t·ª•c ƒë·∫•u th·∫ßu ƒë·∫•t x√¢y d·ª±ng\", \"Th·ªß t·ª•c c·∫•p l·∫°i l√Ω l·ªãch t∆∞ ph√°p\", \"Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng\", \"Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü\", \"Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc\", \"Th·ªß t·ª•c ƒëƒÉng k√Ω l·∫°i k·∫øt h√¥n\", \"Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n c√≥ y·∫øu t·ªë n∆∞·ªõc ngo√†i\", \"Th·ªß t·ª•c l√†m gi·∫•y khai sinh\", \"Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 1 th√†nh vi√™n\", \"Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 2 th√†nh vi√™n tr·ªü l√™n\", \"Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£\", \"Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p t·ªânh\"]\n",
    "_TEST_QUERIES = [\n",
    "    \"Chuy·ªÉn Tr∆∞·ªùng\", \n",
    "    \"Chuyen Truong\", \n",
    "    \"Khai Sinh\", \n",
    "    \"Ch√°u mu·ªën chuy·ªÉn tr∆∞·ªùng c·∫•p 3 th√¨ c·∫ßn ph·∫£i l√†m g√¨?\", \n",
    "    \"T√¥i mu·ªën m·ªü c√¥ng ty th√¨ th·ªß t·ª•c g√¨?\", \n",
    "    \"kh·ªüi nghi·ªáp\", \n",
    "    \"S·∫Øp c∆∞·ªõi v·ª£ c·∫ßn l√†m g√¨?\", \n",
    "    \"ƒëƒÉng k√Ω k·∫øt h√¥n\", \n",
    "    \"dƒÇng  k√ù  k√ät   hoN\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7dd27",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download as HF_Download\n",
    "from tokenizers import Tokenizer as STL_Tokenizer\n",
    "from rank_bm25 import BM25Okapi as BM25_Retriever\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pkg.NLPT.NLPT import Process_NLPT_Tokenize, Process_NLPT_Normalize\n",
    "os.makedirs(\"_hyse\", exist_ok=True)\n",
    "\n",
    "def dict2json(dict, jsonpath):\n",
    "    try:\n",
    "        with open(jsonpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict, f, ensure_ascii=False, indent=4)\n",
    "    except Exception as er:\n",
    "        print(f\"‚ö†Ô∏è dict2json > Error: {er}\")\n",
    "\n",
    "def json2dict(jsonpath):\n",
    "    dict = {}\n",
    "    try:\n",
    "        with open(jsonpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            dict = json.load(f)\n",
    "    except Exception as er:\n",
    "        print(f\"‚ö†Ô∏è json2dict > Error: {er}\")\n",
    "    return dict\n",
    "\n",
    "class SentenceTransformerLite:\n",
    "    # Init: model_path -> model + tokenizer\n",
    "    def __init__(self, model_path=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        try:\n",
    "            # Model (ONNX)\n",
    "            try: HF_Download(repo_id=model_path, filename=\"onnx/model.onnx_data\")\n",
    "            except: pass\n",
    "            STL_model = ort.InferenceSession(HF_Download(repo_id=model_path, filename=\"onnx/model.onnx\"))\n",
    "            # Tokenizer\n",
    "            STL_tokenizer = STL_Tokenizer.from_pretrained(model_path)\n",
    "            STL_tokenizer.enable_padding(pad_id=1, pad_token=\"<pad>\")\n",
    "            STL_tokenizer.enable_truncation(max_length=512)\n",
    "        except Exception as er:\n",
    "            raise ValueError(f\"‚ö†Ô∏è > SentenceTransformerLite > init > Error: {er}\")\n",
    "        # Return\n",
    "        self.STL_model = STL_model\n",
    "        self.STL_tokenizer = STL_tokenizer\n",
    "    # Encode: Text(s) -> Embedding(s)\n",
    "    def encode(self, inputtexts):\n",
    "        # Ensure inputtexts is a list of strings\n",
    "        if isinstance(inputtexts, list) and all(isinstance(e, str) for e in inputtexts):\n",
    "            if len(inputtexts) == 0:\n",
    "                raise ValueError(f\"‚ö†Ô∏è > SentenceTransformerLite > encode > inputtexts = empty list []\")\n",
    "        elif isinstance(inputtexts, str):\n",
    "            inputtexts = [inputtexts]\n",
    "        else:\n",
    "            raise ValueError(f\"‚ö†Ô∏è > SentenceTransformerLite > encode > inputtexts != string or list of strings\")\n",
    "        # Tokenize\n",
    "        inputs = self.STL_tokenizer.encode_batch(inputtexts, is_pretokenized=False)\n",
    "        inputs_ids = np.array([e.ids for e in inputs], dtype=np.int64)\n",
    "        inputs_msk = np.array([e.attention_mask for e in inputs], dtype=np.int64)\n",
    "        # Encoding\n",
    "        embeddings = self.STL_model.run(None, {\"input_ids\": inputs_ids, \"attention_mask\": inputs_msk})[0]                                             # Encode\n",
    "        embeddings = np.sum(embeddings * np.expand_dims(inputs_msk, axis=-1), axis=1) / np.maximum(np.sum(inputs_msk, axis=1, keepdims=True), 1e-9)   # Pooling\n",
    "        embeddings = embeddings / np.maximum(np.linalg.norm(embeddings, axis=1, keepdims=True), 1e-9)                                                 # Normalize\n",
    "        # Return\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HYSE_EngineSemantic:\n",
    "    # # ----- Example -----\n",
    "    # engine_semantic = HYSE_EngineSemantic()\n",
    "    # engine_semantic.update(_TEST_PASSAGES)\n",
    "    # engine_semantic.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_sem1\", modelpath=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        self.name = name\n",
    "        self.modelpath = modelpath\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.npy\"\n",
    "        self.model = SentenceTransformerLite(modelpath)\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # üì§ Read file as docs\n",
    "            self.embs = np.load(self.savepath_embs)            # üì§ Read file as embs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            print(f\"> HYSE_EngineSemantic '{self.name}' > No new docs\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"> HYSE_EngineSemantic '{self.name}' > Updating new docs...\")\n",
    "            self.docs = new_docs\n",
    "            self.embs = self.model.encode(self.docs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # üì• Save docs as file\n",
    "            np.save(self.savepath_embs, self.embs)             # üì• Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        embs_queries = self.model.encode(new_queries)\n",
    "        # -----\n",
    "        similarities = embs_queries @ self.embs.T\n",
    "        best_matching_idxs = [[idx for idx, _ in sorted(enumerate(sim), key=lambda x: x[1], reverse=True)][:min(top, len(self.docs))] for sim in similarities]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        res = [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "        return [[e for e in q if e[\"score\"] > 0] for q in res]\n",
    "\n",
    "class HYSE_EngineLexical:\n",
    "    # # ----- Example -----\n",
    "    # engine_lexical = HYSE_EngineLexical()\n",
    "    # engine_lexical.update(_TEST_PASSAGES)\n",
    "    # engine_lexical.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_lex1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        self.model = None\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # üì§ Read file as docs\n",
    "            self.embs = json2dict(self.savepath_embs)[\"embs\"]  # üì§ Read file as embs\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            print(f\"> HYSE_EngineLexical '{self.name}' > No new docs\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"> HYSE_EngineLexical '{self.name}' > Updating new docs...\")\n",
    "            self.docs = new_docs\n",
    "            self.embs = [Process_NLPT_Tokenize(e) for e in self.docs]\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # üì• Save docs as file\n",
    "            dict2json({\"embs\": self.embs}, self.savepath_embs) # üì• Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        queries_embs = [Process_NLPT_Tokenize(e) for e in new_queries]\n",
    "        # -----\n",
    "        similarities = [self.model.get_scores(query_emb) for query_emb in queries_embs]\n",
    "        best_matching_idxs = [self.model.get_top_n(query_emb, range(len(self.docs)), n=top) for query_emb in queries_embs]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        res = [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "        return [[e for e in q if e[\"score\"] > 0] for q in res]\n",
    "\n",
    "class HYSE_EngineExactMatch:\n",
    "    # # ----- Example -----\n",
    "    # engine_exactmatch = HYSE_EngineExactMatch()\n",
    "    # engine_exactmatch.update(_TEST_PASSAGES)\n",
    "    # engine_exactmatch.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_exa1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        if os.path.exists(self.savepath_docs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # üì§ Read file as docs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            print(f\"> HYSE_EngineExactMatch '{self.name}' > No new docs\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"> HYSE_EngineExactMatch '{self.name}' > Updating new docs...\")\n",
    "            self.docs = new_docs\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # üì• Save docs as file\n",
    "    def search(self, new_queries):\n",
    "        best_matching_idxs = []\n",
    "        for q in new_queries:\n",
    "            # Exact match with diacritics\n",
    "            tmp_idxs = [i for i, d in enumerate(self.docs) if q.lower().strip() in d.lower().strip()]\n",
    "            if len(tmp_idxs) == 0:\n",
    "                # Exact match without diacritics\n",
    "                tmp_idxs = [i for i, d in enumerate(self.docs) if Process_NLPT_Normalize(q) in Process_NLPT_Normalize(d)]\n",
    "            best_matching_idxs.append(tmp_idxs)\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[round(len(new_queries[qidx])/len(doc), 3) for doc in e] for qidx, e in enumerate(best_matching_docs)]\n",
    "        # -----\n",
    "        res = [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "        return [[e for e in q if e[\"score\"] > 0] for q in res]\n",
    "\n",
    "class HYSE_EngineHybrid:\n",
    "    # # ----- Example -----\n",
    "    # hyse_engine = HYSE_EngineHybrid()\n",
    "    # hyse_engine.update(_TEST_PASSAGES)\n",
    "    # hyse_engine.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"HYSE1\"):\n",
    "        self.search_engine_1 = HYSE_EngineExactMatch(name=f\"{name}_EXA1\")\n",
    "        self.search_engine_2 = HYSE_EngineLexical(name=f\"{name}_LEX1\")\n",
    "        self.search_engine_3 = HYSE_EngineSemantic(name=f\"{name}_SEM1\", modelpath=\"onelevelstudio/ML-E5-0.3B\")\n",
    "        self.search_engine_4 = HYSE_EngineSemantic(name=f\"{name}_SEM2\", modelpath=\"onelevelstudio/MPNET-0.3B\")\n",
    "    def update(self, new_docs):\n",
    "        self.search_engine_1.update(new_docs)\n",
    "        self.search_engine_2.update(new_docs)\n",
    "        self.search_engine_3.update(new_docs)\n",
    "        self.search_engine_4.update(new_docs)\n",
    "    def search(self, new_queries):\n",
    "        res_1 = self.search_engine_1.search(new_queries)\n",
    "        res_2 = self.search_engine_2.search(new_queries)\n",
    "        res_3 = self.search_engine_3.search(new_queries)\n",
    "        res_4 = self.search_engine_4.search(new_queries)\n",
    "        res_search = []\n",
    "        for i in range(len(res_1)):\n",
    "            # ---------- Case 1: Exact match\n",
    "            if len(res_1[i]) > 0:\n",
    "                tmp_res = res_1[i]\n",
    "            # ---------- Case 2: Not exact match\n",
    "            else:\n",
    "                res_234 = res_2[i]+res_3[i]+res_4[i]\n",
    "                res_234 = [{\"index\": e[\"index\"], \"doc\": e[\"doc\"]} for e in res_234]\n",
    "                doc_counts = {}\n",
    "                for item in res_234: doc_counts[item['doc']] = doc_counts.get(item['doc'], 0) + 1\n",
    "                tmp_res = [{'index': item['index'], 'doc': doc, 'score': doc_counts[doc]} for doc, item in {d['doc']: d for d in res_234}.items()]\n",
    "                tmp_res = sorted(tmp_res, key=lambda x: (-x['score'], len(x['doc'])))\n",
    "            # ----------\n",
    "            res_search.append(tmp_res)\n",
    "        return res_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605fed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_semantic = HYSE_EngineSemantic(name=\"a\", modelpath=\"onelevelstudio/ML-E5-0.3B\")\n",
    "engine_semantic.update(_TEST_PASSAGES)\n",
    "engine_semantic.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_semantic = HYSE_EngineSemantic(name=\"b\", modelpath=\"onelevelstudio/MPNET-0.3B\")\n",
    "engine_semantic.update(_TEST_PASSAGES)\n",
    "engine_semantic.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e4d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_lexical = HYSE_EngineLexical(name=\"c\")\n",
    "engine_lexical.update(_TEST_PASSAGES)\n",
    "engine_lexical.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_exactmatch = HYSE_EngineExactMatch(name=\"d\")\n",
    "engine_exactmatch.update(_TEST_PASSAGES)\n",
    "engine_exactmatch.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyse_engine = HYSE_EngineHybrid(name=\"e\")\n",
    "hyse_engine.update(_TEST_PASSAGES)\n",
    "hyse_engine.search(_TEST_QUERIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3651012",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_queries = [\"dƒÇng  k√ù  k√ät   hoN  \\n   hello   \\t\\n\\n\\t    yeah!!!\\t...   '     \\n\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
