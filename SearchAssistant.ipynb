{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39065967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"static/DATA.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#     DATA = json.load(f)\n",
    "# ldg_names = [e[\"name\"] for e in DATA[\"DVC_TTHC_LamDong\"][\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff17d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_PASSAGES = [\"Thá»§ tá»¥c thÃ nh láº­p cÃ´ng ty tÆ° nhÃ¢n\", \"Thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n\", \"Thá»§ tá»¥c chuyá»ƒn nhÆ°á»£ng quyá»n sá»­ dá»¥ng Ä‘áº¥t\", \"Thá»§ tá»¥c Ä‘áº¥u tháº§u Ä‘áº¥t xÃ¢y dá»±ng\", \"Thá»§ tá»¥c cáº¥p láº¡i lÃ½ lá»‹ch tÆ° phÃ¡p\", \"Thá»§ tá»¥c chuyá»ƒn trÆ°á»ng cho há»c sinh trung há»c phá»• thÃ´ng\", \"Thá»§ tá»¥c chuyá»ƒn trÆ°á»ng cho há»c sinh trung há»c cÆ¡ sá»Ÿ\", \"Thá»§ tá»¥c chuyá»ƒn trÆ°á»ng cho há»c sinh tiá»ƒu há»c\", \"Thá»§ tá»¥c Ä‘Äƒng kÃ½ láº¡i káº¿t hÃ´n\", \"Thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n cÃ³ yáº¿u tá»‘ nÆ°á»›c ngoÃ i\", \"Thá»§ tá»¥c lÃ m giáº¥y khai sinh\", \"Thá»§ tá»¥c thÃ nh láº­p cÃ´ng ty trÃ¡ch nhiá»‡m há»¯u háº¡n 1 thÃ nh viÃªn\", \"Thá»§ tá»¥c thÃ nh láº­p cÃ´ng ty trÃ¡ch nhiá»‡m há»¯u háº¡n 2 thÃ nh viÃªn trá»Ÿ lÃªn\", \"Thá»§ tá»¥c tá»‘ cÃ¡o táº¡i cáº¥p xÃ£\", \"Thá»§ tá»¥c tá»‘ cÃ¡o táº¡i cáº¥p tá»‰nh\"]\n",
    "_TEST_QUERIES = [\"ChÃ¡u muá»‘n chuyá»ƒn trÆ°á»ng cáº¥p 3 thÃ¬ cáº§n pháº£i lÃ m gÃ¬?\", \"TÃ´i muá»‘n má»Ÿ cÃ´ng ty thÃ¬ thá»§ tá»¥c gÃ¬?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7dd27",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download as HF_Download\n",
    "from tokenizers import Tokenizer as STL_Tokenizer\n",
    "from rank_bm25 import BM25Okapi as BM25_Retriever\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "os.makedirs(\"_hyse\", exist_ok=True)\n",
    "\n",
    "def dict2json(dict, jsonpath):\n",
    "    try:\n",
    "        with open(jsonpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict, f, ensure_ascii=False, indent=4)\n",
    "    except Exception as er:\n",
    "        print(f\"âš ï¸ dict2json > Error: {er}\")\n",
    "\n",
    "def json2dict(jsonpath):\n",
    "    dict = {}\n",
    "    try:\n",
    "        with open(jsonpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            dict = json.load(f)\n",
    "    except Exception as er:\n",
    "        print(f\"âš ï¸ json2dict > Error: {er}\")\n",
    "    return dict\n",
    "\n",
    "class SentenceTransformerLite:\n",
    "    # Init: model_path -> model + tokenizer\n",
    "    def __init__(self, model_path=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        try:\n",
    "            # Model (ONNX)\n",
    "            try: HF_Download(repo_id=model_path, filename=\"onnx/model.onnx_data\")\n",
    "            except: pass\n",
    "            STL_model = ort.InferenceSession(HF_Download(repo_id=model_path, filename=\"onnx/model.onnx\"))\n",
    "            # Tokenizer\n",
    "            STL_tokenizer = STL_Tokenizer.from_pretrained(model_path)\n",
    "            STL_tokenizer.enable_padding(pad_id=1, pad_token=\"<pad>\")\n",
    "            STL_tokenizer.enable_truncation(max_length=512)\n",
    "        except Exception as er:\n",
    "            raise ValueError(f\"âš ï¸ > SentenceTransformerLite > init > Error: {er}\")\n",
    "        # Return\n",
    "        self.STL_model = STL_model\n",
    "        self.STL_tokenizer = STL_tokenizer\n",
    "    # Encode: Text(s) -> Embedding(s)\n",
    "    def encode(self, inputtexts):\n",
    "        # Ensure inputtexts is a list of strings\n",
    "        if isinstance(inputtexts, list) and all(isinstance(e, str) for e in inputtexts):\n",
    "            if len(inputtexts) == 0:\n",
    "                raise ValueError(f\"âš ï¸ > SentenceTransformerLite > encode > inputtexts = empty list []\")\n",
    "        elif isinstance(inputtexts, str):\n",
    "            inputtexts = [inputtexts]\n",
    "        else:\n",
    "            raise ValueError(f\"âš ï¸ > SentenceTransformerLite > encode > inputtexts != string or list of strings\")\n",
    "        # Tokenize\n",
    "        inputs = self.STL_tokenizer.encode_batch(inputtexts, is_pretokenized=False)\n",
    "        inputs_ids = np.array([e.ids for e in inputs], dtype=np.int64)\n",
    "        inputs_msk = np.array([e.attention_mask for e in inputs], dtype=np.int64)\n",
    "        # Encoding\n",
    "        embeddings = self.STL_model.run(None, {\"input_ids\": inputs_ids, \"attention_mask\": inputs_msk})[0]                                             # Encode\n",
    "        embeddings = np.sum(embeddings * np.expand_dims(inputs_msk, axis=-1), axis=1) / np.maximum(np.sum(inputs_msk, axis=1, keepdims=True), 1e-9)   # Pooling\n",
    "        embeddings = embeddings / np.maximum(np.linalg.norm(embeddings, axis=1, keepdims=True), 1e-9)                                                 # Normalize\n",
    "        # Return\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcc1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineSemantic:\n",
    "    # ----- Example -----\n",
    "    # engine_semantic = EngineSemantic()\n",
    "    # engine_semantic.update(_TEST_PASSAGES)\n",
    "    # engine_semantic.search(_TEST_QUERIES)\n",
    "    # -------------------\n",
    "    def __init__(self, name=\"hyse1_sem1\", modelpath=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        self.name = name\n",
    "        self.modelpath = modelpath\n",
    "        self.savepath_docs = f\"_hyse/{name}.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}.npy\"\n",
    "        self.model = SentenceTransformerLite(modelpath)\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # ðŸ“¤ Read file as docs\n",
    "            self.embs = np.load(self.savepath_embs)            # ðŸ“¤ Read file as embs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            pass\n",
    "        else:\n",
    "            self.docs = new_docs\n",
    "            self.embs = self.model.encode(self.docs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # ðŸ“¥ Save docs as file\n",
    "            np.save(self.savepath_embs, self.embs)             # ðŸ“¥ Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        embs_queries = self.model.encode(new_queries)\n",
    "        # -----\n",
    "        similarities = embs_queries @ self.embs.T\n",
    "        best_matching_idxs = [[idx for idx, _ in sorted(enumerate(sim), key=lambda x: x[1], reverse=True)][:min(top, len(self.docs))] for sim in similarities]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        return [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
