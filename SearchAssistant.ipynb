{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39065967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"static/DATA.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#     DATA = json.load(f)\n",
    "# ldg_names = [e[\"name\"] for e in DATA[\"DVC_TTHC_LamDong\"][\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff17d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_PASSAGES = [\"Thá»§ tá»¥c thÃ nh láº­p cÃ´ng ty tÆ° nhÃ¢n\", \"Thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n\", \"Thá»§ tá»¥c chuyá»ƒn nhÆ°á»£ng quyá»n sá»­ dá»¥ng Ä‘áº¥t\", \"Thá»§ tá»¥c Ä‘áº¥u tháº§u Ä‘áº¥t xÃ¢y dá»±ng\", \"Thá»§ tá»¥c cáº¥p láº¡i lÃ½ lá»‹ch tÆ° phÃ¡p\", \"Thá»§ tá»¥c chuyá»ƒn trÆ°á»ng cho há»c sinh trung há»c phá»• thÃ´ng\", \"Thá»§ tá»¥c chuyá»ƒn trÆ°á»ng cho há»c sinh trung há»c cÆ¡ sá»Ÿ\", \"Thá»§ tá»¥c chuyá»ƒn trÆ°á»ng cho há»c sinh tiá»ƒu há»c\", \"Thá»§ tá»¥c Ä‘Äƒng kÃ½ láº¡i káº¿t hÃ´n\", \"Thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n cÃ³ yáº¿u tá»‘ nÆ°á»›c ngoÃ i\", \"Thá»§ tá»¥c lÃ m giáº¥y khai sinh\", \"Thá»§ tá»¥c thÃ nh láº­p cÃ´ng ty trÃ¡ch nhiá»‡m há»¯u háº¡n 1 thÃ nh viÃªn\", \"Thá»§ tá»¥c thÃ nh láº­p cÃ´ng ty trÃ¡ch nhiá»‡m há»¯u háº¡n 2 thÃ nh viÃªn trá»Ÿ lÃªn\", \"Thá»§ tá»¥c tá»‘ cÃ¡o táº¡i cáº¥p xÃ£\", \"Thá»§ tá»¥c tá»‘ cÃ¡o táº¡i cáº¥p tá»‰nh\"]\n",
    "_TEST_QUERIES = [\n",
    "    \"Chuyá»ƒn TrÆ°á»ng\", \n",
    "    \"Chuyen Truong\", \n",
    "    \"Khai Sinh\", \n",
    "    \"ChÃ¡u muá»‘n chuyá»ƒn trÆ°á»ng cáº¥p 3 thÃ¬ cáº§n pháº£i lÃ m gÃ¬?\", \n",
    "    \"TÃ´i muá»‘n má»Ÿ cÃ´ng ty thÃ¬ thá»§ tá»¥c gÃ¬?\", \n",
    "    \"khá»Ÿi nghiá»‡p\", \n",
    "    \"Sáº¯p cÆ°á»›i vá»£ cáº§n lÃ m gÃ¬?\", \n",
    "    \"Ä‘Äƒng kÃ½ káº¿t hÃ´n\", \n",
    "    \" \\t\\t\\n\\n dÄ‚ng  kÃ  kÃŠt \\n\\t\\n  hoN\\n        \"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7dd27",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edb4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download as HF_Download\n",
    "from tokenizers import Tokenizer as STL_Tokenizer\n",
    "from rank_bm25 import BM25Okapi as BM25_Retriever\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pkg.NLPT.NLPT import NLPT_Tokenize, NLPT_Normalize\n",
    "os.makedirs(\"_hyse\", exist_ok=True)\n",
    "\n",
    "def dict2json(dict, jsonpath):\n",
    "    try:\n",
    "        with open(jsonpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict, f, ensure_ascii=False, indent=4)\n",
    "    except Exception as er:\n",
    "        print(f\"âš ï¸ dict2json > Error: {er}\")\n",
    "\n",
    "def json2dict(jsonpath):\n",
    "    dict = {}\n",
    "    try:\n",
    "        with open(jsonpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            dict = json.load(f)\n",
    "    except Exception as er:\n",
    "        print(f\"âš ï¸ json2dict > Error: {er}\")\n",
    "    return dict\n",
    "\n",
    "def list2batches(ls, batch_size=5):\n",
    "    return [ls[i:i + batch_size] for i in range(0, len(ls), batch_size)]\n",
    "\n",
    "class SentenceTransformerLite:\n",
    "    # Init: model_path -> model + tokenizer\n",
    "    def __init__(self, model_path=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        try:\n",
    "            # Model (ONNX)\n",
    "            try: HF_Download(repo_id=model_path, filename=\"onnx/model.onnx_data\")\n",
    "            except: pass\n",
    "            STL_model = ort.InferenceSession(HF_Download(repo_id=model_path, filename=\"onnx/model.onnx\"))\n",
    "            # Tokenizer\n",
    "            STL_tokenizer = STL_Tokenizer.from_pretrained(model_path)\n",
    "            STL_tokenizer.enable_padding(pad_id=1, pad_token=\"<pad>\")\n",
    "            STL_tokenizer.enable_truncation(max_length=512)\n",
    "        except Exception as er:\n",
    "            raise ValueError(f\"âš ï¸ > SentenceTransformerLite > init > Error: {er}\")\n",
    "        # Return\n",
    "        self.STL_model = STL_model\n",
    "        self.STL_tokenizer = STL_tokenizer\n",
    "    # Encode: Text(s) -> Embedding(s)\n",
    "    def encode(self, inputtexts):\n",
    "        # Ensure inputtexts is a list of strings\n",
    "        if isinstance(inputtexts, list) and all(isinstance(e, str) for e in inputtexts):\n",
    "            if len(inputtexts) == 0:\n",
    "                raise ValueError(f\"âš ï¸ > SentenceTransformerLite > encode > inputtexts = empty list []\")\n",
    "        elif isinstance(inputtexts, str):\n",
    "            inputtexts = [inputtexts]\n",
    "        else:\n",
    "            raise ValueError(f\"âš ï¸ > SentenceTransformerLite > encode > inputtexts != string or list of strings\")\n",
    "        # Tokenize\n",
    "        inputs = self.STL_tokenizer.encode_batch(inputtexts, is_pretokenized=False)\n",
    "        inputs_ids = np.array([e.ids for e in inputs], dtype=np.int64)\n",
    "        inputs_msk = np.array([e.attention_mask for e in inputs], dtype=np.int64)\n",
    "        # Encoding\n",
    "        embeddings = self.STL_model.run(None, {\"input_ids\": inputs_ids, \"attention_mask\": inputs_msk})[0]                                             # Encode\n",
    "        embeddings = np.sum(embeddings * np.expand_dims(inputs_msk, axis=-1), axis=1) / np.maximum(np.sum(inputs_msk, axis=1, keepdims=True), 1e-9)   # Pooling\n",
    "        embeddings = embeddings / np.maximum(np.linalg.norm(embeddings, axis=1, keepdims=True), 1e-9)                                                 # Normalize\n",
    "        # Return\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcc1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queries_preprocessing(new_queries):\n",
    "    return [NLPT_Normalize(e, replace_spacelikes_with_1space=True) for e in new_queries] # ðŸŒ Opinionated input queries pre-processing\n",
    "\n",
    "class HYSE_EngineSemantic:\n",
    "    # # ----- Example -----\n",
    "    # engine_semantic = HYSE_EngineSemantic(name=\"a\", modelpath=\"onelevelstudio/ML-E5-0.3B\")\n",
    "    # engine_semantic.update(_TEST_PASSAGES)\n",
    "    # engine_semantic.search(_TEST_QUERIES)\n",
    "    # engine_semantic = HYSE_EngineSemantic(name=\"b\", modelpath=\"onelevelstudio/MPNET-0.3B\")\n",
    "    # engine_semantic.update(_TEST_PASSAGES)\n",
    "    # engine_semantic.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_sem1\", modelpath=\"onelevelstudio/ML-E5-0.3B\", stl_encoding_batch_size=100):\n",
    "        self.name = name\n",
    "        self.modelpath = modelpath\n",
    "        self.stl_encoding_batch_size = stl_encoding_batch_size\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.npy\"\n",
    "        self.model = SentenceTransformerLite(modelpath)\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # ðŸ“¤ Read file as docs\n",
    "            self.embs = np.load(self.savepath_embs)            # ðŸ“¤ Read file as embs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            print(f\"> HYSE_EngineSemantic '{self.name}' > No new docs\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"> HYSE_EngineSemantic '{self.name}' > Updating new docs...\")\n",
    "            self.docs = new_docs\n",
    "            # ----------\n",
    "            # ---------- STL Encoding: list of strings -> list of embs\n",
    "            # ----------\n",
    "            # self.embs = self.model.encode(self.docs)                                          # Original - will suffer when the self.docs is too large\n",
    "            docs_batches = list2batches(self.docs, batch_size=self.stl_encoding_batch_size)\n",
    "            embs_batches = []\n",
    "            for iii, docsbatch in enumerate(docs_batches):\n",
    "                print(f\"> HYSE_EngineSemantic '{self.name}' > Encoding batch {iii+1}/{len(docs_batches)}...\")\n",
    "                embsbatch = self.model.encode(docsbatch)\n",
    "                embs_batches.append(embsbatch)\n",
    "            self.embs = np.concatenate(embs_batches, axis=0)\n",
    "            # ----------\n",
    "            # ----------\n",
    "            # ----------\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # ðŸ“¥ Save docs as file\n",
    "            np.save(self.savepath_embs, self.embs)             # ðŸ“¥ Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        new_queries = queries_preprocessing(new_queries)\n",
    "        # -----\n",
    "        embs_queries = self.model.encode(new_queries)\n",
    "        # -----\n",
    "        similarities = embs_queries @ self.embs.T\n",
    "        best_matching_idxs = [[idx for idx, _ in sorted(enumerate(sim), key=lambda x: x[1], reverse=True)][:min(top, len(self.docs))] for sim in similarities]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        res = [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "        return [[e for e in q if e[\"score\"] > 0] for q in res]\n",
    "\n",
    "class HYSE_EngineLexical:\n",
    "    # # ----- Example -----\n",
    "    # engine_lexical = HYSE_EngineLexical(name=\"c\")\n",
    "    # engine_lexical.update(_TEST_PASSAGES)\n",
    "    # engine_lexical.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_lex1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        self.model = None\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # ðŸ“¤ Read file as docs\n",
    "            self.embs = json2dict(self.savepath_embs)[\"embs\"]  # ðŸ“¤ Read file as embs\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            print(f\"> HYSE_EngineLexical '{self.name}' > No new docs\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"> HYSE_EngineLexical '{self.name}' > Updating new docs...\")\n",
    "            self.docs = new_docs\n",
    "            self.embs = [NLPT_Tokenize(e) for e in self.docs]\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # ðŸ“¥ Save docs as file\n",
    "            dict2json({\"embs\": self.embs}, self.savepath_embs) # ðŸ“¥ Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        new_queries = queries_preprocessing(new_queries)\n",
    "        # -----\n",
    "        queries_embs = [NLPT_Tokenize(e) for e in new_queries]\n",
    "        # -----\n",
    "        similarities = [self.model.get_scores(query_emb) for query_emb in queries_embs]\n",
    "        best_matching_idxs = [self.model.get_top_n(query_emb, range(len(self.docs)), n=top) for query_emb in queries_embs]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        res = [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "        return [[e for e in q if e[\"score\"] > 0] for q in res]\n",
    "\n",
    "class HYSE_EngineExactMatch:\n",
    "    # # ----- Example -----\n",
    "    # engine_exactmatch = HYSE_EngineExactMatch(name=\"d\")\n",
    "    # engine_exactmatch.update(_TEST_PASSAGES)\n",
    "    # engine_exactmatch.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_exa1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        if os.path.exists(self.savepath_docs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # ðŸ“¤ Read file as docs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            print(f\"> HYSE_EngineExactMatch '{self.name}' > No new docs\")\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"> HYSE_EngineExactMatch '{self.name}' > Updating new docs...\")\n",
    "            self.docs = new_docs\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # ðŸ“¥ Save docs as file\n",
    "    def search(self, new_queries):\n",
    "        new_queries = queries_preprocessing(new_queries)\n",
    "        # -----\n",
    "        best_matching_idxs = []\n",
    "        for q in new_queries:\n",
    "            # Exact match with diacritics\n",
    "            tmp_idxs = [i for i, d in enumerate(self.docs) if NLPT_Normalize(q, lower=True) in NLPT_Normalize(d, lower=True)]\n",
    "            if len(tmp_idxs) == 0:\n",
    "                # Exact match without diacritics\n",
    "                tmp_idxs = [i for i, d in enumerate(self.docs) if NLPT_Normalize(q, lower=True, remove_diacritics=True) in NLPT_Normalize(d, lower=True, remove_diacritics=True)]\n",
    "            best_matching_idxs.append(tmp_idxs)\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[round(len(new_queries[qidx])/len(doc), 3) for doc in e] for qidx, e in enumerate(best_matching_docs)]\n",
    "        # -----\n",
    "        res = [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "        return [[e for e in q if e[\"score\"] > 0] for q in res]\n",
    "\n",
    "class HYSE_EngineHybrid:\n",
    "    # # ----- Example -----\n",
    "    # hyse_engine = HYSE_EngineHybrid(name=\"e\")\n",
    "    # hyse_engine.update(_TEST_PASSAGES)\n",
    "    # hyse_engine.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"HYSE1\"):\n",
    "        self.search_engine_1 = HYSE_EngineExactMatch(name=f\"{name}_EXA1\")\n",
    "        self.search_engine_2 = HYSE_EngineLexical(name=f\"{name}_LEX1\")\n",
    "        self.search_engine_3 = HYSE_EngineSemantic(name=f\"{name}_SEM1\", modelpath=\"onelevelstudio/ML-E5-0.3B\")\n",
    "        self.search_engine_4 = HYSE_EngineSemantic(name=f\"{name}_SEM2\", modelpath=\"onelevelstudio/MPNET-0.3B\")\n",
    "    def update(self, new_docs):\n",
    "        self.search_engine_1.update(new_docs)\n",
    "        self.search_engine_2.update(new_docs)\n",
    "        self.search_engine_3.update(new_docs)\n",
    "        self.search_engine_4.update(new_docs)\n",
    "    def search(self, new_queries):\n",
    "        res_1 = self.search_engine_1.search(new_queries)\n",
    "        res_2 = self.search_engine_2.search(new_queries)\n",
    "        res_3 = self.search_engine_3.search(new_queries)\n",
    "        res_4 = self.search_engine_4.search(new_queries)\n",
    "        # -----\n",
    "        res_search = []\n",
    "        for i in range(len(res_1)):\n",
    "            # ---------- Case 1: Exact match\n",
    "            if len(res_1[i]) > 0:\n",
    "                tmp_res = res_1[i]\n",
    "            # ---------- Case 2: Not exact match\n",
    "            else:\n",
    "                res_234 = res_2[i]+res_3[i]+res_4[i]\n",
    "                res_234 = [{\"index\": e[\"index\"], \"doc\": e[\"doc\"]} for e in res_234]\n",
    "                doc_counts = {}\n",
    "                for item in res_234: doc_counts[item['doc']] = doc_counts.get(item['doc'], 0) + 1\n",
    "                tmp_res = [{'index': item['index'], 'doc': doc, 'score': doc_counts[doc]} for doc, item in {d['doc']: d for d in res_234}.items()]\n",
    "            # ----------\n",
    "            tmp_res = sorted(tmp_res, key=lambda x: (-x['score'], len(x['doc']))) # Sort by score (higher first), then sort by length (shorter first)\n",
    "            res_search.append(tmp_res)\n",
    "        return res_search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
