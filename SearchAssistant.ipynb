{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39065967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"static/DATA.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#     DATA = json.load(f)\n",
    "# ldg_names = [e[\"name\"] for e in DATA[\"DVC_TTHC_LamDong\"][\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff17d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_PASSAGES = [\"Thá»§ tá»¥c thÃ nh láº­p cÃ´ng ty tÆ° nhÃ¢n\", \"Thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n\", \"Thá»§ tá»¥c chuyá»ƒn nhÆ°á»£ng quyá»n sá»­ dá»¥ng Ä‘áº¥t\", \"Thá»§ tá»¥c Ä‘áº¥u tháº§u Ä‘áº¥t xÃ¢y dá»±ng\", \"Thá»§ tá»¥c cáº¥p láº¡i lÃ½ lá»‹ch tÆ° phÃ¡p\", \"Thá»§ tá»¥c chuyá»ƒn trÆ°á»ng cho há»c sinh trung há»c phá»• thÃ´ng\", \"Thá»§ tá»¥c chuyá»ƒn trÆ°á»ng cho há»c sinh trung há»c cÆ¡ sá»Ÿ\", \"Thá»§ tá»¥c chuyá»ƒn trÆ°á»ng cho há»c sinh tiá»ƒu há»c\", \"Thá»§ tá»¥c Ä‘Äƒng kÃ½ láº¡i káº¿t hÃ´n\", \"Thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n cÃ³ yáº¿u tá»‘ nÆ°á»›c ngoÃ i\", \"Thá»§ tá»¥c lÃ m giáº¥y khai sinh\", \"Thá»§ tá»¥c thÃ nh láº­p cÃ´ng ty trÃ¡ch nhiá»‡m há»¯u háº¡n 1 thÃ nh viÃªn\", \"Thá»§ tá»¥c thÃ nh láº­p cÃ´ng ty trÃ¡ch nhiá»‡m há»¯u háº¡n 2 thÃ nh viÃªn trá»Ÿ lÃªn\", \"Thá»§ tá»¥c tá»‘ cÃ¡o táº¡i cáº¥p xÃ£\", \"Thá»§ tá»¥c tá»‘ cÃ¡o táº¡i cáº¥p tá»‰nh\"]\n",
    "_TEST_QUERIES = [\"ChÃ¡u muá»‘n chuyá»ƒn trÆ°á»ng cáº¥p 3 thÃ¬ cáº§n pháº£i lÃ m gÃ¬?\", \"TÃ´i muá»‘n má»Ÿ cÃ´ng ty thÃ¬ thá»§ tá»¥c gÃ¬?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7dd27",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edb4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download as HF_Download\n",
    "from tokenizers import Tokenizer as STL_Tokenizer\n",
    "from rank_bm25 import BM25Okapi as BM25_Retriever\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pkg.NLPT.NLPT import Process_NLPT_Tokenize, Process_NLPT_Normalize\n",
    "os.makedirs(\"_hyse\", exist_ok=True)\n",
    "\n",
    "def dict2json(dict, jsonpath):\n",
    "    try:\n",
    "        with open(jsonpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict, f, ensure_ascii=False, indent=4)\n",
    "    except Exception as er:\n",
    "        print(f\"âš ï¸ dict2json > Error: {er}\")\n",
    "\n",
    "def json2dict(jsonpath):\n",
    "    dict = {}\n",
    "    try:\n",
    "        with open(jsonpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            dict = json.load(f)\n",
    "    except Exception as er:\n",
    "        print(f\"âš ï¸ json2dict > Error: {er}\")\n",
    "    return dict\n",
    "\n",
    "class SentenceTransformerLite:\n",
    "    # Init: model_path -> model + tokenizer\n",
    "    def __init__(self, model_path=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        try:\n",
    "            # Model (ONNX)\n",
    "            try: HF_Download(repo_id=model_path, filename=\"onnx/model.onnx_data\")\n",
    "            except: pass\n",
    "            STL_model = ort.InferenceSession(HF_Download(repo_id=model_path, filename=\"onnx/model.onnx\"))\n",
    "            # Tokenizer\n",
    "            STL_tokenizer = STL_Tokenizer.from_pretrained(model_path)\n",
    "            STL_tokenizer.enable_padding(pad_id=1, pad_token=\"<pad>\")\n",
    "            STL_tokenizer.enable_truncation(max_length=512)\n",
    "        except Exception as er:\n",
    "            raise ValueError(f\"âš ï¸ > SentenceTransformerLite > init > Error: {er}\")\n",
    "        # Return\n",
    "        self.STL_model = STL_model\n",
    "        self.STL_tokenizer = STL_tokenizer\n",
    "    # Encode: Text(s) -> Embedding(s)\n",
    "    def encode(self, inputtexts):\n",
    "        # Ensure inputtexts is a list of strings\n",
    "        if isinstance(inputtexts, list) and all(isinstance(e, str) for e in inputtexts):\n",
    "            if len(inputtexts) == 0:\n",
    "                raise ValueError(f\"âš ï¸ > SentenceTransformerLite > encode > inputtexts = empty list []\")\n",
    "        elif isinstance(inputtexts, str):\n",
    "            inputtexts = [inputtexts]\n",
    "        else:\n",
    "            raise ValueError(f\"âš ï¸ > SentenceTransformerLite > encode > inputtexts != string or list of strings\")\n",
    "        # Tokenize\n",
    "        inputs = self.STL_tokenizer.encode_batch(inputtexts, is_pretokenized=False)\n",
    "        inputs_ids = np.array([e.ids for e in inputs], dtype=np.int64)\n",
    "        inputs_msk = np.array([e.attention_mask for e in inputs], dtype=np.int64)\n",
    "        # Encoding\n",
    "        embeddings = self.STL_model.run(None, {\"input_ids\": inputs_ids, \"attention_mask\": inputs_msk})[0]                                             # Encode\n",
    "        embeddings = np.sum(embeddings * np.expand_dims(inputs_msk, axis=-1), axis=1) / np.maximum(np.sum(inputs_msk, axis=1, keepdims=True), 1e-9)   # Pooling\n",
    "        embeddings = embeddings / np.maximum(np.linalg.norm(embeddings, axis=1, keepdims=True), 1e-9)                                                 # Normalize\n",
    "        # Return\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcc1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineSemantic:\n",
    "    # ----- Example -----\n",
    "    # engine_semantic = EngineSemantic()\n",
    "    # engine_semantic.update(_TEST_PASSAGES)\n",
    "    # engine_semantic.search(_TEST_QUERIES)\n",
    "    # -------------------\n",
    "    def __init__(self, name=\"hyse1_sem1\", modelpath=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        self.name = name\n",
    "        self.modelpath = modelpath\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.npy\"\n",
    "        self.model = SentenceTransformerLite(modelpath)\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # ðŸ“¤ Read file as docs\n",
    "            self.embs = np.load(self.savepath_embs)            # ðŸ“¤ Read file as embs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            pass\n",
    "        else:\n",
    "            self.docs = new_docs\n",
    "            self.embs = self.model.encode(self.docs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # ðŸ“¥ Save docs as file\n",
    "            np.save(self.savepath_embs, self.embs)             # ðŸ“¥ Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        embs_queries = self.model.encode(new_queries)\n",
    "        # -----\n",
    "        similarities = embs_queries @ self.embs.T\n",
    "        best_matching_idxs = [[idx for idx, _ in sorted(enumerate(sim), key=lambda x: x[1], reverse=True)][:min(top, len(self.docs))] for sim in similarities]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        return [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728cca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineLexical:\n",
    "    def __init__(self, name=\"hyse1_lex1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        self.model = None\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # ðŸ“¤ Read file as docs\n",
    "            self.embs = json2dict(self.savepath_embs)[\"embs\"]  # ðŸ“¤ Read file as embs\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            pass\n",
    "        else:\n",
    "            self.docs = new_docs\n",
    "            self.embs = [Process_NLPT_Tokenize(e) for e in self.docs]\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # ðŸ“¥ Save docs as file\n",
    "            dict2json({\"embs\": self.embs}, self.savepath_embs) # ðŸ“¥ Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        queries_embs = [Process_NLPT_Tokenize(e) for e in new_queries]\n",
    "        similarities = [self.model.get_scores(query_emb) for query_emb in queries_embs]\n",
    "        best_matching_idxs = [self.model.get_top_n(query_emb, range(len(self.docs)), n=top) for query_emb in queries_embs]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        return [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8759bc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m engine_semantic = EngineLexical()\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# engine_semantic.update(_TEST_PASSAGES)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mengine_semantic\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_TEST_QUERIES\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mEngineLexical.search\u001b[39m\u001b[34m(self, new_queries, top)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_queries, top=\u001b[32m5\u001b[39m):\n\u001b[32m     23\u001b[39m     queries_embs = [Process_NLPT_Tokenize(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m new_queries]\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     similarities = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_emb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquery_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mqueries_embs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     25\u001b[39m     best_matching_idxs = [\u001b[38;5;28mself\u001b[39m.model.get_top_n(query_emb, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.docs)), n=top) \u001b[38;5;28;01mfor\u001b[39;00m query_emb \u001b[38;5;129;01min\u001b[39;00m queries_embs]\n\u001b[32m     26\u001b[39m     best_matching_docs = [[\u001b[38;5;28mself\u001b[39m.docs[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m e] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m best_matching_idxs]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_queries, top=\u001b[32m5\u001b[39m):\n\u001b[32m     23\u001b[39m     queries_embs = [Process_NLPT_Tokenize(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m new_queries]\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     similarities = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_scores\u001b[49m(query_emb) \u001b[38;5;28;01mfor\u001b[39;00m query_emb \u001b[38;5;129;01min\u001b[39;00m queries_embs]\n\u001b[32m     25\u001b[39m     best_matching_idxs = [\u001b[38;5;28mself\u001b[39m.model.get_top_n(query_emb, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.docs)), n=top) \u001b[38;5;28;01mfor\u001b[39;00m query_emb \u001b[38;5;129;01min\u001b[39;00m queries_embs]\n\u001b[32m     26\u001b[39m     best_matching_docs = [[\u001b[38;5;28mself\u001b[39m.docs[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m e] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m best_matching_idxs]\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'get_scores'"
     ]
    }
   ],
   "source": [
    "engine_semantic = EngineLexical()\n",
    "# engine_semantic.update(_TEST_PASSAGES)\n",
    "engine_semantic.search(_TEST_QUERIES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
