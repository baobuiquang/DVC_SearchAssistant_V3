{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39065967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"static/DATA.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "#     DATA = json.load(f)\n",
    "# ldg_names = [e[\"name\"] for e in DATA[\"DVC_TTHC_LamDong\"][\"data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff17d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_PASSAGES = [\"Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty t∆∞ nh√¢n\", \"Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n\", \"Th·ªß t·ª•c chuy·ªÉn nh∆∞·ª£ng quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t\", \"Th·ªß t·ª•c ƒë·∫•u th·∫ßu ƒë·∫•t x√¢y d·ª±ng\", \"Th·ªß t·ª•c c·∫•p l·∫°i l√Ω l·ªãch t∆∞ ph√°p\", \"Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng\", \"Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü\", \"Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc\", \"Th·ªß t·ª•c ƒëƒÉng k√Ω l·∫°i k·∫øt h√¥n\", \"Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n c√≥ y·∫øu t·ªë n∆∞·ªõc ngo√†i\", \"Th·ªß t·ª•c l√†m gi·∫•y khai sinh\", \"Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 1 th√†nh vi√™n\", \"Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 2 th√†nh vi√™n tr·ªü l√™n\", \"Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£\", \"Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p t·ªânh\"]\n",
    "_TEST_QUERIES = [\"Chuy·ªÉn Tr∆∞·ªùng\", \"Chuyen Truong\", \"Khai Sinh\", \"Ch√°u mu·ªën chuy·ªÉn tr∆∞·ªùng c·∫•p 3 th√¨ c·∫ßn ph·∫£i l√†m g√¨?\", \"T√¥i mu·ªën m·ªü c√¥ng ty th√¨ th·ªß t·ª•c g√¨?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7dd27",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edb4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download as HF_Download\n",
    "from tokenizers import Tokenizer as STL_Tokenizer\n",
    "from rank_bm25 import BM25Okapi as BM25_Retriever\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pkg.NLPT.NLPT import Process_NLPT_Tokenize, Process_NLPT_Normalize\n",
    "os.makedirs(\"_hyse\", exist_ok=True)\n",
    "\n",
    "def dict2json(dict, jsonpath):\n",
    "    try:\n",
    "        with open(jsonpath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(dict, f, ensure_ascii=False, indent=4)\n",
    "    except Exception as er:\n",
    "        print(f\"‚ö†Ô∏è dict2json > Error: {er}\")\n",
    "\n",
    "def json2dict(jsonpath):\n",
    "    dict = {}\n",
    "    try:\n",
    "        with open(jsonpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            dict = json.load(f)\n",
    "    except Exception as er:\n",
    "        print(f\"‚ö†Ô∏è json2dict > Error: {er}\")\n",
    "    return dict\n",
    "\n",
    "class SentenceTransformerLite:\n",
    "    # Init: model_path -> model + tokenizer\n",
    "    def __init__(self, model_path=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        try:\n",
    "            # Model (ONNX)\n",
    "            try: HF_Download(repo_id=model_path, filename=\"onnx/model.onnx_data\")\n",
    "            except: pass\n",
    "            STL_model = ort.InferenceSession(HF_Download(repo_id=model_path, filename=\"onnx/model.onnx\"))\n",
    "            # Tokenizer\n",
    "            STL_tokenizer = STL_Tokenizer.from_pretrained(model_path)\n",
    "            STL_tokenizer.enable_padding(pad_id=1, pad_token=\"<pad>\")\n",
    "            STL_tokenizer.enable_truncation(max_length=512)\n",
    "        except Exception as er:\n",
    "            raise ValueError(f\"‚ö†Ô∏è > SentenceTransformerLite > init > Error: {er}\")\n",
    "        # Return\n",
    "        self.STL_model = STL_model\n",
    "        self.STL_tokenizer = STL_tokenizer\n",
    "    # Encode: Text(s) -> Embedding(s)\n",
    "    def encode(self, inputtexts):\n",
    "        # Ensure inputtexts is a list of strings\n",
    "        if isinstance(inputtexts, list) and all(isinstance(e, str) for e in inputtexts):\n",
    "            if len(inputtexts) == 0:\n",
    "                raise ValueError(f\"‚ö†Ô∏è > SentenceTransformerLite > encode > inputtexts = empty list []\")\n",
    "        elif isinstance(inputtexts, str):\n",
    "            inputtexts = [inputtexts]\n",
    "        else:\n",
    "            raise ValueError(f\"‚ö†Ô∏è > SentenceTransformerLite > encode > inputtexts != string or list of strings\")\n",
    "        # Tokenize\n",
    "        inputs = self.STL_tokenizer.encode_batch(inputtexts, is_pretokenized=False)\n",
    "        inputs_ids = np.array([e.ids for e in inputs], dtype=np.int64)\n",
    "        inputs_msk = np.array([e.attention_mask for e in inputs], dtype=np.int64)\n",
    "        # Encoding\n",
    "        embeddings = self.STL_model.run(None, {\"input_ids\": inputs_ids, \"attention_mask\": inputs_msk})[0]                                             # Encode\n",
    "        embeddings = np.sum(embeddings * np.expand_dims(inputs_msk, axis=-1), axis=1) / np.maximum(np.sum(inputs_msk, axis=1, keepdims=True), 1e-9)   # Pooling\n",
    "        embeddings = embeddings / np.maximum(np.linalg.norm(embeddings, axis=1, keepdims=True), 1e-9)                                                 # Normalize\n",
    "        # Return\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcc1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HYSE_EngineSemantic:\n",
    "    # # ----- Example -----\n",
    "    # engine_semantic = HYSE_EngineSemantic()\n",
    "    # engine_semantic.update(_TEST_PASSAGES)\n",
    "    # engine_semantic.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_sem1\", modelpath=\"onelevelstudio/ML-E5-0.3B\"):\n",
    "        self.name = name\n",
    "        self.modelpath = modelpath\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.npy\"\n",
    "        self.model = SentenceTransformerLite(modelpath)\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # üì§ Read file as docs\n",
    "            self.embs = np.load(self.savepath_embs)            # üì§ Read file as embs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            pass\n",
    "        else:\n",
    "            self.docs = new_docs\n",
    "            self.embs = self.model.encode(self.docs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # üì• Save docs as file\n",
    "            np.save(self.savepath_embs, self.embs)             # üì• Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        embs_queries = self.model.encode(new_queries)\n",
    "        # -----\n",
    "        similarities = embs_queries @ self.embs.T\n",
    "        best_matching_idxs = [[idx for idx, _ in sorted(enumerate(sim), key=lambda x: x[1], reverse=True)][:min(top, len(self.docs))] for sim in similarities]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        return [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "\n",
    "class HYSE_EngineLexical:\n",
    "    # # ----- Example -----\n",
    "    # engine_lexical = HYSE_EngineLexical()\n",
    "    # engine_lexical.update(_TEST_PASSAGES)\n",
    "    # engine_lexical.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_lex1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        self.savepath_embs = f\"_hyse/{name}_embs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        self.embs = []\n",
    "        self.model = None\n",
    "        if os.path.exists(self.savepath_docs) and os.path.exists(self.savepath_embs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # üì§ Read file as docs\n",
    "            self.embs = json2dict(self.savepath_embs)[\"embs\"]  # üì§ Read file as embs\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            pass\n",
    "        else:\n",
    "            self.docs = new_docs\n",
    "            self.embs = [Process_NLPT_Tokenize(e) for e in self.docs]\n",
    "            self.model = BM25_Retriever(self.embs)\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # üì• Save docs as file\n",
    "            dict2json({\"embs\": self.embs}, self.savepath_embs) # üì• Save embs as file\n",
    "    def search(self, new_queries, top=5):\n",
    "        queries_embs = [Process_NLPT_Tokenize(e) for e in new_queries]\n",
    "        # -----\n",
    "        similarities = [self.model.get_scores(query_emb) for query_emb in queries_embs]\n",
    "        best_matching_idxs = [self.model.get_top_n(query_emb, range(len(self.docs)), n=top) for query_emb in queries_embs]\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[similarities[i][idx] for idx in idxs] for i, idxs in enumerate(best_matching_idxs)]\n",
    "        # -----\n",
    "        return [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]\n",
    "\n",
    "class HYSE_EngineExactMatch:\n",
    "    # # ----- Example -----\n",
    "    # engine_exactmatch = HYSE_EngineExactMatch()\n",
    "    # engine_exactmatch.update(_TEST_PASSAGES)\n",
    "    # engine_exactmatch.search(_TEST_QUERIES)\n",
    "    # # -------------------\n",
    "    def __init__(self, name=\"hyse001_exa1\"):\n",
    "        self.name = name\n",
    "        self.savepath_docs = f\"_hyse/{name}_docs.json\"\n",
    "        # ----------\n",
    "        self.docs = []\n",
    "        if os.path.exists(self.savepath_docs):\n",
    "            self.docs = json2dict(self.savepath_docs)[\"docs\"]  # üì§ Read file as docs\n",
    "    def update(self, new_docs):\n",
    "        if self.docs == new_docs:\n",
    "            pass\n",
    "        else:\n",
    "            self.docs = new_docs\n",
    "            dict2json({\"docs\": self.docs}, self.savepath_docs) # üì• Save docs as file\n",
    "    def search(self, new_queries):\n",
    "        best_matching_idxs = []\n",
    "        for q in new_queries:\n",
    "            # Exact match with diacritics\n",
    "            tmp_idxs = [i for i, d in enumerate(self.docs) if q.lower().strip() in d.lower().strip()]\n",
    "            if len(tmp_idxs) == 0:\n",
    "                # Exact match without diacritics\n",
    "                tmp_idxs = [i for i, d in enumerate(self.docs) if Process_NLPT_Normalize(q) in Process_NLPT_Normalize(d)]\n",
    "            best_matching_idxs.append(tmp_idxs)\n",
    "        best_matching_docs = [[self.docs[idx] for idx in e] for e in best_matching_idxs]\n",
    "        best_matching_similarities = [[round(len(new_queries[qidx])/len(doc), 3) for doc in e] for qidx, e in enumerate(best_matching_docs)]\n",
    "        # -----\n",
    "        return [[{\"index\": ee[0], \"doc\": ee[1], \"score\": round(float(ee[2]), 3)} for ee in zip(e[0], e[1], e[2])] for e in zip(best_matching_idxs, best_matching_docs, best_matching_similarities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8f2d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HYSE_EngineHybrid:\n",
    "    def __init__(self, name=\"HYSE1\"):\n",
    "        self.search_engine_1 = HYSE_EngineSemantic(name=f\"{name}_SEM1\", modelpath=\"onelevelstudio/ML-E5-0.3B\")\n",
    "        self.search_engine_2 = HYSE_EngineSemantic(name=f\"{name}_SEM2\", modelpath=\"onelevelstudio/MPNET-0.3B\")\n",
    "        self.search_engine_3 = HYSE_EngineLexical(name=f\"{name}_LEX1\")\n",
    "        self.search_engine_4 = HYSE_EngineExactMatch(name=f\"{name}_EXA1\")\n",
    "        self.docs = [\"‚ú®\"]\n",
    "        self.update(self.docs)\n",
    "    def update(self, new_docs):\n",
    "        self.docs = new_docs\n",
    "        self.search_engine_1.update(self.docs)\n",
    "        self.search_engine_2.update(self.docs)\n",
    "        self.search_engine_3.update(self.docs)\n",
    "        self.search_engine_4.update(self.docs)\n",
    "    def search(self, new_queries):\n",
    "        res_1 = self.search_engine_1.search(new_queries)\n",
    "        res_2 = self.search_engine_2.search(new_queries)\n",
    "        res_3 = self.search_engine_3.search(new_queries)\n",
    "        res_4 = self.search_engine_4.search(new_queries)\n",
    "        print(res_1)\n",
    "        print(res_2)\n",
    "        print(res_3)\n",
    "        print(res_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c5cf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'index': 7, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc', 'score': 0.894}, {'index': 5, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng', 'score': 0.884}, {'index': 6, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü', 'score': 0.88}, {'index': 2, 'doc': 'Th·ªß t·ª•c chuy·ªÉn nh∆∞·ª£ng quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t', 'score': 0.867}, {'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.821}], [{'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.777}, {'index': 13, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£', 'score': 0.773}, {'index': 1, 'doc': 'Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n', 'score': 0.773}, {'index': 14, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p t·ªânh', 'score': 0.768}, {'index': 0, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty t∆∞ nh√¢n', 'score': 0.768}], [{'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.841}, {'index': 1, 'doc': 'Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n', 'score': 0.802}, {'index': 8, 'doc': 'Th·ªß t·ª•c ƒëƒÉng k√Ω l·∫°i k·∫øt h√¥n', 'score': 0.795}, {'index': 0, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty t∆∞ nh√¢n', 'score': 0.785}, {'index': 9, 'doc': 'Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n c√≥ y·∫øu t·ªë n∆∞·ªõc ngo√†i', 'score': 0.781}], [{'index': 7, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc', 'score': 0.902}, {'index': 5, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng', 'score': 0.89}, {'index': 6, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü', 'score': 0.887}, {'index': 2, 'doc': 'Th·ªß t·ª•c chuy·ªÉn nh∆∞·ª£ng quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t', 'score': 0.844}, {'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.825}], [{'index': 0, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty t∆∞ nh√¢n', 'score': 0.924}, {'index': 12, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 2 th√†nh vi√™n tr·ªü l√™n', 'score': 0.883}, {'index': 11, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 1 th√†nh vi√™n', 'score': 0.879}, {'index': 1, 'doc': 'Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n', 'score': 0.868}, {'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.863}]]\n",
      "[[{'index': 6, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü', 'score': 0.809}, {'index': 5, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng', 'score': 0.803}, {'index': 7, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc', 'score': 0.8}, {'index': 13, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£', 'score': 0.467}, {'index': 4, 'doc': 'Th·ªß t·ª•c c·∫•p l·∫°i l√Ω l·ªãch t∆∞ ph√°p', 'score': 0.424}], [{'index': 7, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc', 'score': 0.159}, {'index': 6, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü', 'score': 0.136}, {'index': 5, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng', 'score': 0.133}, {'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.114}, {'index': 13, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£', 'score': 0.082}], [{'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.184}, {'index': 12, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 2 th√†nh vi√™n tr·ªü l√™n', 'score': 0.131}, {'index': 11, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 1 th√†nh vi√™n', 'score': 0.118}, {'index': 13, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£', 'score': 0.111}, {'index': 4, 'doc': 'Th·ªß t·ª•c c·∫•p l·∫°i l√Ω l·ªãch t∆∞ ph√°p', 'score': 0.103}], [{'index': 5, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng', 'score': 0.773}, {'index': 6, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü', 'score': 0.762}, {'index': 7, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc', 'score': 0.694}, {'index': 13, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£', 'score': 0.248}, {'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.208}], [{'index': 0, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty t∆∞ nh√¢n', 'score': 0.72}, {'index': 11, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 1 th√†nh vi√™n', 'score': 0.566}, {'index': 12, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 2 th√†nh vi√™n tr·ªü l√™n', 'score': 0.548}, {'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.403}, {'index': 1, 'doc': 'Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n', 'score': 0.369}]]\n",
      "[[{'index': 5, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng', 'score': 2.483}, {'index': 6, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü', 'score': 2.483}, {'index': 7, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc', 'score': 2.483}, {'index': 11, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 1 th√†nh vi√™n', 'score': 0.0}, {'index': 14, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p t·ªânh', 'score': 0.0}], [{'index': 14, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p t·ªânh', 'score': 0.0}, {'index': 13, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£', 'score': 0.0}, {'index': 12, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 2 th√†nh vi√™n tr·ªü l√™n', 'score': 0.0}, {'index': 11, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 1 th√†nh vi√™n', 'score': 0.0}, {'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.0}], [{'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 2.716}, {'index': 13, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£', 'score': 0.0}, {'index': 14, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p t·ªânh', 'score': 0.0}, {'index': 12, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 2 th√†nh vi√™n tr·ªü l√™n', 'score': 0.0}, {'index': 11, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 1 th√†nh vi√™n', 'score': 0.0}], [{'index': 5, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng', 'score': 2.483}, {'index': 6, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü', 'score': 2.483}, {'index': 7, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc', 'score': 2.483}, {'index': 13, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p x√£', 'score': 1.368}, {'index': 14, 'doc': 'Th·ªß t·ª•c t·ªë c√°o t·∫°i c·∫•p t·ªânh', 'score': 1.368}], [{'index': 0, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty t∆∞ nh√¢n', 'score': 1.846}, {'index': 11, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 1 th√†nh vi√™n', 'score': 1.413}, {'index': 12, 'doc': 'Th·ªß t·ª•c th√†nh l·∫≠p c√¥ng ty tr√°ch nhi·ªám h·ªØu h·∫°n 2 th√†nh vi√™n tr·ªü l√™n', 'score': 1.311}, {'index': 1, 'doc': 'Th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n', 'score': 0.532}, {'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.532}]]\n",
      "[[{'index': 5, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng', 'score': 0.241}, {'index': 6, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü', 'score': 0.26}, {'index': 7, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc', 'score': 0.302}], [{'index': 5, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc ph·ªï th√¥ng', 'score': 0.241}, {'index': 6, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh trung h·ªçc c∆° s·ªü', 'score': 0.26}, {'index': 7, 'doc': 'Th·ªß t·ª•c chuy·ªÉn tr∆∞·ªùng cho h·ªçc sinh ti·ªÉu h·ªçc', 'score': 0.302}], [{'index': 10, 'doc': 'Th·ªß t·ª•c l√†m gi·∫•y khai sinh', 'score': 0.346}], [], []]\n"
     ]
    }
   ],
   "source": [
    "# ----- Example -----\n",
    "hyse_engine = HYSE_EngineHybrid()\n",
    "hyse_engine.update(_TEST_PASSAGES)\n",
    "hyse_engine.search(_TEST_QUERIES)\n",
    "# -------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
