from pkg.HYSE.HYSE import HYSE_EngineHybrid, json2dict, dict2json
from pkg.NLPT.NLPT import NLPT_Normalize
from pkg.LLM.LLM import Process_LLM

from static.DATA_HARDPARAPHRASE import DATA_HARDPARAPHRASE
from static.DATA_HARDCODE import DATA_HARDCODE

import numpy as np
import logging
import json
import re
import os

os.makedirs("_log", exist_ok=True)
file_handler = logging.FileHandler('_log/log.txt', encoding='utf-8')
logging.basicConfig(
    handlers=[file_handler],
    level=logging.INFO,
    format='%(asctime)s | %(message)s',
    datefmt='%y%m%d%H%M%S'
)

# ====================================================================================================
# ====================================================================================================
# ====================================================================================================

def print_dict(d, indent=4):
    if not isinstance(d, dict):
        print("Input is not a dictionary.")
        return
    pretty_string = json.dumps(d, indent=indent, ensure_ascii=False)
    print(pretty_string)

def print_list(l):
    for e in l:
        print(e)

# CONVERT: ['h∆∞·ªõng d·∫´n', 'h·ªì s∆°']
# TO:      ['h∆∞·ªõng d·∫´n', 'h∆∞·ªõng dan', 'huong d·∫´n', 'huong dan', 'h·ªì s∆°', 'h·ªì so', 'ho s∆°', 'ho so']
def create_normalied_list_of_text(myls):
    def combine_lists_with_spaces(list1, list2):
        result = []
        def backtrack(index=0, current=""):
            if index == len(list1):
                result.append(current.strip()) 
                return
            sep = " " if index < len(list1) - 1 else ""
            backtrack(index + 1, current + list1[index] + sep)
            backtrack(index + 1, current + list2[index] + sep)
        backtrack()
        return result
    res = [item for sublist in [combine_lists_with_spaces(ele.split(), [NLPT_Normalize(el, lower=True, remove_diacritics=True, replace_spacelikes_with_1space=True, remove_punctuations=True) for el in ele.split()]) for ele in myls] for item in sublist]
    return res

def create_prompt_1(inputtext, hyse_res):
    prompt_list_1 = [{"T√™n th·ªß t·ª•c": e['doc'], "INDEX": str(e['index'])} for e in hyse_res]
    prompt_schema_1 = """\
{
    "type": "object",
    "properties": {
        "T√™n th·ªß t·ª•c": {"type": "string", "description": "T√™n c·ªßa th·ªß t·ª•c li√™n quan nh·∫•t"}
        "INDEX": {"type": "string", "description": "INDEX c·ªßa th·ªß t·ª•c li√™n quan nh·∫•t"},
    }
}"""
    prompt_1 = f"""\
B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p: (1) C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng, (2) Danh s√°ch th·ªß t·ª•c hi·ªán c√≥, v√† (3) Schema c·∫•u tr√∫c c·ªßa k·∫øt qu·∫£.
Nhi·ªám v·ª• c·ªßa b·∫°n l√†: (4) Tr√≠ch xu·∫•t duy nh·∫•t 1 th·ªß t·ª•c li√™n quan nh·∫•t ƒë·∫øn c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.

### (1) C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:
"{inputtext}"

### (2) Danh s√°ch th·ªß t·ª•c hi·ªán c√≥:
{prompt_list_1}

### (3) Schema c·∫•u tr√∫c c·ªßa k·∫øt qu·∫£:
{prompt_schema_1}

### (4) Nhi·ªám v·ª•:
T·ª´ c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng "{inputtext}", t√¨m ra duy nh·∫•t 1 th·ªß t·ª•c li√™n quan nh·∫•t ƒë·∫øn c√¢u h·ªèi, tu√¢n th·ªß schema m·ªôt c√°ch ch√≠nh x√°c.
L∆∞u √Ω quan tr·ªçng: N·∫øu kh√¥ng c√≥ th·ªß t·ª•c n√†o li√™n quan, tr·∫£ v·ªÅ "Kh√¥ng c√≥ th·ªß t·ª•c li√™n quan".
ƒê·ªãnh d·∫°ng k·∫øt qu·∫£: Kh√¥ng gi·∫£i th√≠ch, kh√¥ng b√¨nh lu·∫≠n, kh√¥ng vƒÉn b·∫£n th·ª´a. Ch·ªâ tr·∫£ v·ªÅ k·∫øt qu·∫£ JSON h·ª£p l·ªá. B·∫Øt ƒë·∫ßu b·∫±ng "{{", k·∫øt th√∫c b·∫±ng "}}".
"""
    return prompt_1

def preprocess_fieldtext_html_in_DATA(DATA):
    def preprocess_fieldtext_html(fld_text):
        fld_text = fld_text.replace("<br/>", "")                             # Pre-processing: Remove all <br/>
        fld_text = re.sub(r'<strong[^>]*>(.*?)</strong>', r'\1', fld_text)   # Pre-processing: Remove <strong> tag (keep content)
        fld_text = re.sub(r'<span[^>]*>(.*?)</span>', r'\1', fld_text)       # Pre-processing: Remove <span> tag (keep content)
        fld_text = re.sub(r'<em[^>]*>(.*?)</em>', r'\1', fld_text)           # Pre-processing: Remove <em> tag (keep content)
        fld_text = re.sub(r'<h1[^>]*>(.*?)</h1>', r'\1', fld_text)           # Pre-processing: Remove <h1> tag (keep content)
        fld_text = re.sub(r'<h2[^>]*>(.*?)</h2>', r'\1', fld_text)           # Pre-processing: Remove <h2> tag (keep content)
        fld_text = re.sub(r'<h3[^>]*>(.*?)</h4>', r'\1', fld_text)           # Pre-processing: Remove <h3> tag (keep content)
        fld_text = re.sub(r'<h4[^>]*>(.*?)</h4>', r'\1', fld_text)           # Pre-processing: Remove <h4> tag (keep content)
        fld_text = re.sub(r'<h5[^>]*>(.*?)</h5>', r'\1', fld_text)           # Pre-processing: Remove <h5> tag (keep content)
        fld_text = re.sub(r'<h6[^>]*>(.*?)</h6>', r'\1', fld_text)           # Pre-processing: Remove <h6> tag (keep content)
        fld_text = re.sub(r'<i[^>]*>(.*?)</i>', r'\1', fld_text)             # Pre-processing: Remove <i> tag (keep content)
        fld_text = re.sub(r'<b[^>]*>(.*?)</b>', r'\1', fld_text)             # Pre-processing: Remove <b> tag (keep content)
        fld_text = re.sub(r'<a[^>]*>(.*?)</a>', r'\1', fld_text)             # Pre-processing: Remove <a> tag (keep content)
        fld_text = re.sub(r'<u[^>]*>(.*?)</u>', r'\1', fld_text)             # Pre-processing: Remove <u> tag (keep content)
        fld_text = re.sub(r"\s*class=['\"][^'\"]*['\"]", '', fld_text)       # Pre-processing: Remove class="something"
        fld_text = re.sub(r"\s*style=['\"][^'\"]*['\"]", '', fld_text)       # Pre-processing: Remove style="something"
        return fld_text
    for infopool_id in list(DATA.keys()):
        for iii, thutuc in enumerate(DATA[infopool_id]["data"]):
            fieldnames = list(thutuc["content"].keys())
            for fld in fieldnames:
                DATA[infopool_id]["data"][iii]["content"][fld] = preprocess_fieldtext_html(thutuc["content"][fld])
    return DATA

# ====================================================================================================
# ====================================================================================================
# ====================================================================================================

# Read DATA
DATA = json2dict("static/DATA.json")
DATA = preprocess_fieldtext_html_in_DATA(DATA)

INFOPOOL_DATAS = {}
INFOPOOL_HYSE_ENGINES = {}
for infopool_id in list(DATA.keys()):
    INFOPOOL_DATAS[infopool_id] = DATA[infopool_id]["data"]
    INFOPOOL_HYSE_ENGINES[infopool_id] = HYSE_EngineHybrid(name=infopool_id)
    INFOPOOL_HYSE_ENGINES[infopool_id].update([e["name"] for e in DATA[infopool_id]["data"]])

# ====================================================================================================
# ====================================================================================================
# ====================================================================================================

def find_bestthutuc_and_suggestions(inputtext, infopool_id):
    CURRENT_INFOPOOL_ID = infopool_id
    hyse_res = INFOPOOL_HYSE_ENGINES[CURRENT_INFOPOOL_ID].search([inputtext])[0]
    bestthutuc = {}
    suggestions = []
    for _ in range(3):
        llmres1 = Process_LLM(create_prompt_1(inputtext, hyse_res))
        regexmatch1 = re.search(r'\{.*\}', llmres1, re.S)
        if regexmatch1:
            try:
                jsonobj1 = json.loads(regexmatch1.group())
                bestthutuc = INFOPOOL_DATAS[CURRENT_INFOPOOL_ID][int(jsonobj1["INDEX"])]
                if bestthutuc["name"] == jsonobj1["T√™n th·ªß t·ª•c"]:
                    # -------------------------------------------------- # bestthutuc
                    # print_dict(bestthutuc)
                    # -------------------------------------------------- # suggestions
                    MIN_SIM_TO_BE_SUGGESTED = 0.92
                    hyse_res_embs = np.array([INFOPOOL_HYSE_ENGINES[CURRENT_INFOPOOL_ID].search_engine_3.embs[e2] for e2 in [e1["index"] for e1 in hyse_res]])
                    similarities = hyse_res_embs @ hyse_res_embs.T
                    bestthutuc_pos = [e3["doc"] for e3 in hyse_res].index(bestthutuc["name"])
                    suggestions = [INFOPOOL_DATAS[CURRENT_INFOPOOL_ID][hyse_res[ii]["index"]] for ii, sim in sorted(enumerate(similarities[bestthutuc_pos]), key=lambda x: -x[1]) if ii != bestthutuc_pos and sim > MIN_SIM_TO_BE_SUGGESTED]
                    suggestions = [{"name": e4["name"], "link": e4["link"], "code": e4["code"]} for e4 in suggestions]
                    # print_list(suggestions)
                    # -------------------------------------------------- # 
                    break
            except:
                pass
    return bestthutuc, suggestions

def create_api_content_data(bestthutuc):
    fieldnames1 = list(bestthutuc["content"].keys())
    fieldnames2 = [NLPT_Normalize(e, lower=True, remove_diacritics=True, replace_spacelikes_with_1space=True, remove_punctuations=True).replace(" ", "_") for e in fieldnames1]
    content_data = {}
    for fld1, fld2 in zip(fieldnames1, fieldnames2):
        content_data[fld2] = bestthutuc["content"][fld1]
    return content_data

def create_api_content_0(inputtext, bestthutuc):
    OPINIONATED_FIELD_TRIGGERS = [
        { "fieldnames": ['Th√†nh ph·∫ßn h·ªì s∆°'], "triggers": ['th√†nh ph·∫ßn h·ªì s∆°', 'th√†nh ph·∫ßn', 'h·ªì s∆°'] },
        { "fieldnames": ['C√°ch th·ª©c th·ª±c hi·ªán'], "triggers": ['c√°ch th·ª©c th·ª±c hi·ªán', 'c√°ch th·ª©c', 'th·ª±c hi·ªán'] },
        { "fieldnames": ['Tr√¨nh t·ª± th·ª±c hi·ªán'], "triggers": ['tr√¨nh t·ª± th·ª±c hi·ªán', 'tr√¨nh t·ª±', 'th·ª±c hi·ªán'] },
        { "fieldnames": ['Th·ªùi gian gi·∫£i quy·∫øt', 'Th·ªùi h·∫°n gi·∫£i quy·∫øt'], "triggers": ['th·ªùi gian gi·∫£i quy·∫øt', 'th·ªùi h·∫°n gi·∫£i quy·∫øt', 'th·ªùi gian', 'th·ªùi h·∫°n'] },
        { "fieldnames": ['Y√™u c·∫ßu - ƒëi·ªÅu ki·ªán', 'Y√™u c·∫ßu, ƒëi·ªÅu ki·ªán'], "triggers": ['y√™u c·∫ßu - ƒëi·ªÅu ki·ªán', 'y√™u c·∫ßu, ƒëi·ªÅu ki·ªán', 'y√™u c·∫ßu', 'ƒëi·ªÅu ki·ªán'] },
        { "fieldnames": ['ƒê·ªëi t∆∞·ª£ng th·ª±c hi·ªán'], "triggers": ['ƒë·ªëi t∆∞·ª£ng th·ª±c hi·ªán', 'ƒë·ªëi t∆∞·ª£ng', 'th·ª±c hi·ªán'] },
        { "fieldnames": ['CƒÉn c·ª© ph√°p l√Ω'], "triggers": ['cƒÉn c·ª© ph√°p l√Ω', 'cƒÉn c·ª©', 'ph√°p l√Ω'] },
        { "fieldnames": ['Bi·ªÉu m·∫´u ƒë√≠nh k√®m', 'T√™n m·∫´u ƒë∆°n, t·ªù khai'], "triggers": ['bi·ªÉu m·∫´u ƒë√≠nh k√®m', 't√™n m·∫´u ƒë∆°n, t·ªù khai', 'bi·ªÉu m·∫´u', 'm·∫´u ƒë∆°n', 't·ªù khai'] },
        { "fieldnames": ['Ph√≠, l·ªá ph√≠', 'L·ªá Ph√≠', 'Ph√≠'], "triggers": ['ph√≠, l·ªá ph√≠', 'l·ªá ph√≠', 'chi ph√≠', 'bao nhi√™u ti·ªÅn'] },
        { "fieldnames": ['Lƒ©nh v·ª±c'], "triggers": ['lƒ©nh v·ª±c'] },
        { "fieldnames": ['C∆° quan th·ª±c hi·ªán'], "triggers": ['c∆° quan th·ª±c hi·ªán', 'th·ª±c hi·ªán'] },
        { "fieldnames": ['K·∫øt qu·∫£ th·ª±c hi·ªán', 'K·∫øt qu·∫£'], "triggers": ['k·∫øt qu·∫£ th·ª±c hi·ªán', 'k·∫øt qu·∫£', 'th·ª±c hi·ªán'] },
        { "fieldnames": ['ƒê·ªãa ch·ªâ ti·∫øp nh·∫≠n'], "triggers": ['ƒë·ªãa ch·ªâ ti·∫øp nh·∫≠n', 'n∆°i ti·∫øp nh·∫≠n', 'ti·∫øp nh·∫≠n'] },
        { "fieldnames": ['S·ªë l∆∞·ª£ng b·ªô h·ªì s∆°'], "triggers": ['s·ªë l∆∞·ª£ng b·ªô h·ªì s∆°', 's·ªë l∆∞·ª£ng', 'h·ªì s∆°'] },
    ]
    flag_there_is_trigger = False
    TRIGGERED_FIELDS = []
    for ee in OPINIONATED_FIELD_TRIGGERS:
        for trigger in create_normalied_list_of_text(ee["triggers"]):
            if trigger.lower() in inputtext.lower():
                for fld in ee["fieldnames"]:
                    if fld in list(bestthutuc["content"].keys()):
                        TRIGGERED_FIELDS.append(fld)
                flag_there_is_trigger = True
                break
    # ----------------------------------------------------------------------------------------------------
    if flag_there_is_trigger:
        MAXWORDS = 9999
        SELECTED_FIELDS = TRIGGERED_FIELDS
    else:
        MAXWORDS = 100
        SELECTED_FIELDS = [
            "Th√†nh ph·∫ßn h·ªì s∆°", 
            "Tr√¨nh t·ª± th·ª±c hi·ªán", 
            "C√°ch th·ª©c th·ª±c hi·ªán", 
            "Th·ªùi gian gi·∫£i quy·∫øt", "Th·ªùi h·∫°n gi·∫£i quy·∫øt",
            "Ph√≠, l·ªá ph√≠", "Ph√≠", "L·ªá Ph√≠",
            # "K·∫øt qu·∫£", "K·∫øt qu·∫£ th·ª±c hi·ªán",
        ]
    XEMCHITIET = f"""... <a href='{bestthutuc['link']}' target='_blank'>(xem chi ti·∫øt ‚Üó)</a>"""
    content_0 = ""
    content_0 += f"""<a href='{bestthutuc['link']}' target='_blank'><h2>Th·ªß t·ª•c: {bestthutuc['name']}</h2></a>"""
    for fld in SELECTED_FIELDS:
        if fld in list(bestthutuc["content"].keys()):
            # ----------
            bestthutuc_content_fld = bestthutuc["content"][fld]
            # ----------
            content_0 += f"""<h3>{fld}</h3>"""
            content_0 += "<p>"
            if len(bestthutuc_content_fld.split(" ")) < MAXWORDS:
                content_0 += f"""{bestthutuc_content_fld}"""
            else:
                content_0 += f"""{" ".join(bestthutuc_content_fld.split(" ")[:MAXWORDS])}"""
                # ----- # Gradio fix for trimmed text
                if "<ul>" in bestthutuc_content_fld:    content_0 += "</ul>"
                if "<ol>" in bestthutuc_content_fld:    content_0 += "</ol>"
                if "<table>" in bestthutuc_content_fld: content_0 += "</td></tr></tbody></table>"
                # -----
                content_0 += XEMCHITIET
            content_0 += "</p>"
            # -----
    content_0 += f"""<h3>Xem vƒÉn b·∫£n ƒë·∫ßy ƒë·ªß t·∫°i:</h3>"""
    content_0 += f"""<a href='{bestthutuc['link']}' target='_blank'>{bestthutuc['link']}</a>"""
    return content_0

# ====================================================================================================
# ====================================================================================================
# ====================================================================================================

def DVC_SearchAssistant(inputtext, infopool_id):
    # ----- Log üìÑ
    logging.info(f"infopool_id='{infopool_id}' - inputtext='{inputtext}'")
    # -----
    API_OBJECT = {
        "input": inputtext,
        "datapool": infopool_id,
        "name": "",
        "link": "",
        "code": "",
        "content_0": "Xin ch√†o, m√¨nh c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?",
        "content_1": "DO-NOT-USE-THIS",
        "content_2": "DO-NOT-USE-THIS",
        "content_data": {},
        "suggestions": [],
    }
    # --------------------------------------------------
    def inputtext_preprocessing(inputtext):
        return NLPT_Normalize(inputtext, replace_spacelikes_with_1space=True) # üçå Opinionated inputtext pre-processing
    inputtext = inputtext_preprocessing(inputtext)
    # -------------------------------------------------- 1Ô∏è‚É£ Special Case 1: inputtext is empty
    if inputtext == "":
        return API_OBJECT
    # -------------------------------------------------- 2Ô∏è‚É£ Special Case 2A: inputtext contains DATA_HARDPARAPHRASE
    for hardparaphrase in DATA_HARDPARAPHRASE:
        possible_keywords = create_normalied_list_of_text(hardparaphrase["keywords"])
        for k in possible_keywords:
            if k.lower() in inputtext.lower():
                inputtext += " " + hardparaphrase["append_phrase"]
                print(f"> inputtext: {inputtext}")
                break
    # -------------------------------------------------- 2Ô∏è‚É£ Special Case 2B: inputtext is DATA_HARDCODE
    for faq in DATA_HARDCODE[infopool_id]:
        possible_faq_questions = create_normalied_list_of_text(faq["questions"])
        for e in possible_faq_questions:
            if e.lower() in inputtext.lower():
                API_OBJECT = faq["answer"]
                API_OBJECT["input"] = inputtext
                return API_OBJECT
    # -------------------------------------------------- 3Ô∏è‚É£ Case 3: inputtext is normal search text -> LLM
    try:
        bestthutuc, suggestions = find_bestthutuc_and_suggestions(inputtext, infopool_id)
        API_OBJECT["name"] = bestthutuc["name"]
        API_OBJECT["link"] = bestthutuc["link"]
        API_OBJECT["code"] = bestthutuc["code"]
        API_OBJECT["content_0"] = create_api_content_0(inputtext, bestthutuc)
        API_OBJECT["content_1"] = "DO-NOT-USE-THIS"
        API_OBJECT["content_2"] = "DO-NOT-USE-THIS"
        API_OBJECT["content_data"] = create_api_content_data(bestthutuc)
        API_OBJECT["suggestions"] = suggestions
        return API_OBJECT
    except Exception as er:
        print(f"‚ö†Ô∏è DVC_SearchAssistant > Error: {er}")
    # --------------------------------------------------
    return API_OBJECT


# inputtext = "t√¥i mu·ªën c∆∞·ªõi ch·ªìng ng∆∞·ªùi n∆∞·ªõc ngo√†i"
# infopool_id = "DVC_TTHC_LamDong"
# # infopool_id = "DVC_TTHC_LangSon"
# DVC_SearchAssistant(inputtext, infopool_id)